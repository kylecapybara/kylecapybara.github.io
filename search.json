[
  {
    "objectID": "writing/engl110 research/index.html",
    "href": "writing/engl110 research/index.html",
    "title": "Hypertension for Black Americans in Linda Villarosa’s Everything I Thought was Wrong",
    "section": "",
    "text": "This was a reserach essay project for my ENGL110 course “Family, Race, and Kinship”.\n\nIn her essay “Everything I Thought Was Wrong,” Linda Villarosa walks the reader through her experiences with systemic racism in the American healthcare system and her progression of thinking about Black health. “Everything I Thought Was Wrong” is one essay from her book Under the Skin, the culmination of her decades-long career as a health journalist. She recounts her past subscription to a theory about why Black Americans experience higher rates of hypertension, commonly known as high blood pressure. Villarosa explains the theory as such: when crossing the Atlantic in the slave trade, the slaves who survived the ride had a genetic advantage of holding sodium in their system longer, and handed this gene down generation after generation (Villarosa 13). However, this theory lacks scientific backing. Villarosa comments, “no data proves that salt-depleting illnesses like dehydration and diarrhea were responsible for a significant number of deaths on slave ships” (Villarosa 14). She notes research trying to explain the African American health disparity through genomic methods was flawed from conception–one of the theory’s main axioms remains unevidenced. She writes, “Less discussed, often ignored, but also proven is the link between hypertension and stressful environments and situations, including living with bias and discrimination” (Villarosa 14). Villarosa stresses the dissonance between what is allowed in clinical discussions and a factor undoubtedly playing into the issue: systemic racism. The causes of excess Black deaths from hypertension in America are multifaceted, a complex interplay of genetic, socioeconomic, and historical factors no single discipline can fully account for. By not considering other fields’ extramedical explanations for the racialized hypertension disparity in America, like those Linda Villarsoa shares in “Everything I Thought was Wrong,” American hypertension researchers fail to accurately provide a pathway to remedy and perpetuate racist structures through their scholarship.\nHypertension in Black Americans finds specific scholarly importance since, as Villarosa contemplates in her essay, America stands at the forefront of medical and technological advancement, yet Black Americans experience worse health outcomes than people living in third world countries (Villarosa 1). Many current medical researchers focus on excess Black death from cardiovascular disease and search for explanations in genes and patient behavior. Keith Ferdinand and his research group at the Tulane University Medical School looked at the racialized cardiovascular health disparity in America and pointed the finger at poor drug adherence, “When compared with white patients, blacks are disproportionately afflicted by poor adherence to cardiovascular medications” (Ferdinand et al. 1). Ferdinand’s group uses this claim as a starting point for their research and focuses on solving the problem of poor drug adherence by examining different ways health practitioners can encourage adherence like sending text message reminders to take the medicine. But this blames the patient for their hypertension.\nWhile Ferdinand works to improve adherence rates by changing individual patient behavior, Villarosa would argue the patient is not to blame for their hypertension; it is a flaw of a racist system. A pivotal moment happens when she spends the day with Harlem cancer specialist Dr. Freeman. She recounts a Black mother with an accomplished career visiting, “She had a large lump on her left breast … Dr. Freeman asked her gently, without a shred of judgment, only concern, why she hadn’t come to see him sooner, when treatment might have made a difference” (Villarosa 12) Villarosa, as a career long health journalist, knows scholarly medical discourse attributes poor Black health to patient behavior, and uses this story as a platform to talk about medical system distrust and avoidance. When reflecting on the woman Villarosa writes, “This woman had health insurance and access to Dr. Freeman, one of the world’s most renowned cancer specialists and a caring Black physician who chose to practice in Harlem” (Villarosa 12). She watches the mother struggle to get to the doctor even though she clearly has a successful career and financial ability to see Dr. Freeman, but even at the top of the economic spectrum, she hesitates to see him out of fear. During her time at Northeastern University, Chanhyun Park investigated how American hypertension patients avoid the medical system. 28% of patients reported they “would do anything to avoid going to the doctor” (C. Park et al.). This insanely large proportion of (majority Black) patients alone is staggering, but as Park writes in a different paper reviewing literature on hypertension treatment cost-effectiveness, “treatment of hypertension using antihypertensive medicines is cost effective compared with no treatment” (Chanhyun Park et al. 10). Even though scholarship points out the rationality–the cost-effectiveness–of hypertension treatment, nearly 1 in three patients vehemently avoids the path to treatment. Something does not add up.\nVillarosa thinks the medical industry needs to grow past its racism–it needs an intellectual evolution. On an essay length scale, Villarosa depicts her own intellectual evolution. In the past she saw Black health as salt intake and poverty weighing down health outcomes, but now she acknowledges a wide range of socioeconomic statuses found in Black communities, like Freeman’s Harlem, all harmed by a systemically racist medical system. Echoing Villarosa’s sentiments, in a paper about the causes of hypertension in African-Americans, Columbia University Medical Center professor Thomas Pickering wrote, “A huge effort has already been made to try to understand the reasons for the higher prevalence of hypertension in African Americans, almost all of which has involved the underlying assumption that there is some genetically determined physiologic difference” (Pickering 50). Pickering raises a key question without asking a question: how effective can research on this topic be when researchers exploring the subject start off assuming genes cause a significant amount of the disparity? Along similar lines, Audre Lorde, a famous radical Black feminist writer, raises similar questions in her essay “The Master’s Tools Will Never Dismantle the Master’s House,” “What does it mean when the tools of a racist patriarchy are used to examine the fruits of that same racist patriarchy?” (Lorde 94). Lorde, like Pickering, acknowledges the cognitive dissonance of looking for an origin by searching for genes before even asking if the disparity is genomic. While medical scholars only read and reference work from the medical field, they will keep coming up short when trying to explain the difference in hypertension rates.\nRewinding back to medical system avoidance, Villarosa does not end her argument by ascribing Black health disparities in America to system avoidance–the avoidance is, as Audre Lorde might say, the fruit of a racist patriarchy. Medical system avoidance alone cannot explain the racialized difference in hypertension in America. Villarosa builds her case with a story of her father’s hospitalization when battling colon cancer, “Worse, he had restraints on his legs. As we walked in, an attendant was speaking to him in a disrespectful hiss” (Villarosa 18). Through her painful imagery, Villarosa talks about racism inside the healthcare system. Her father was a college educated, government employed man. He was a cancer patient, not a danger to the hospital. When medical researchers, namely hypertension researchers, watch patients fail to consistently come to check up appointments or fail to adhere to their medication and then blame the patient for not following their regime, Villarosa says they should blame themselves.\nWhile Ferdinand’s interpretation of the problem and proposed methods to increase drug adherence in Black patients may be suited for medical discussion, hearing from sociologists is essential to the conversation. University of Michigan professor Jeffrey Morenoff and a group of social science researchers examined hypertension rates in different neighborhoods in Chicago. In most of their statistical models, the group found “no longer significant differences in the odds of hypertension between blacks and whites or across levels of education after adjusting for neighborhood differences” (Morenoff et al. 1859). Morenoff’s study works against the slave survival model; when controlled for surroundings, the hypertension disparity disappeared. Sociologists like Morenoff are interested in how surrounding factors, on many scales, affect health outcomes relating to hypertension. In her essay, Villarosa talks about social factors negatively impacting her own physical and mental health, “In my work at Essence and in my books and lectures, I discussed the heavy toll of being ‘strong’ Black women who sacrifice our physical and emotional health to tend to the well-being of others—children, parents, spouses, lovers, elders, friends, the community itself” (Villarosa 16).\nFor her, the social pressure to be a “strong Black” woman pushed her past the boundaries of her health, just as the surroundings of participants in Morenoff’s study impacted their health. Tying back to hypertension, Pedro Ordúñez-García’s research group investigated racial hypertension differences in Cuba. The history of communist Cuba made their research important to this conversation. Equality finds its place at the center of communist ideology, and in Ordúñez-García’s findings the hypertension rates between Black Cubans and white Cubans were significantly closer than in America. The rates were so similar that by standard statistical practices, the difference in hypertension between Black Cubans and white Cubans would not be significant.\nIn the case of America, less than 60 years ago the US was still in its Jim Crow era characterized by anti-Black laws and a racial caste system all wrapped up a white supremacist bow. In 1959, A few years before the 1964 Civil Rights act started dismantling Jim Crow, Fidel Castro pivoted his Cuba into a communist nation, passing 1,500 pieces of legislation in the first 30 months in power (Benson 24). By the 1980s Black Cubans and white Cubans performed similarly by almost every social metric: high school graduation, life expectancy, and number of professional positions (Benson 24). However, by the 1990s Fidel’s Cuba collapsed into a capitalist system which prioritized giving scarce jobs to white Cubans over Black Cubans (Benson 25). Ordúñez-García’s group posits the Cuban history and culture of equality, and the differing American history of inequality plays a role in the disparity in America, but their study took place in 1998, nearly a decade after communist Cuba’s collapse. A defender of Ordúñez-García may cite the time gap between the collapse of Cuba and the study as small compared to the human life-span. However, racism persisted during communist Cuba’s.\nCastro’s policy made Black Cuban’s nearly identical on paper, but racial prejudice still thrived on the island. The end racism’s own “revolutionary visual materials contradicted themselves by reinforcing ideas of Afro-Cuban immaturity and positioning blacks as clients of the new state” (Benson 29). Afro-Cuban scholarship about Castro’s anti-racism campaign highlights the issue with his policy that also concerns current medical scholarship’s attempts at closing the racialized hypertension divide: interpersonal racism affects health outcomes, just like it affected Villarosa’s father when fighting colon cancer in the hospital.\nExamining a smaller scale, University of Nebraska sociologist Sydney Stahl writes about the psychosocial effect of differing economic status between generations relating to hypertension, “A son who has an occupation which has a higher or lower prestige rank on a hierarchical structure of occupations than his father’s occupation is more likely to have hypertension than is the son whose occupational status is similar to that of his father” (Stahl 108). Stahl talks about a person’s context affecting their blood pressure on a personal scale rather than a country wide scale, like the case of Cuba. African Americans are only, not counting sharecropping, about 6-7 generations away from slavery, the absolute rock bottom of socioeconomic class. African Americans climbing the economic ladder and going to college at higher rates than past generations, Stahl says, increases their likelihood of having higher blood pressure and ultimately their risk of being another of the already too many excess Black deaths in America. The sociological approach broadens the discussion of hypertension in Black Americans to consider the context of a person in their family and the context of their country’s ideological history, like Ordúñez-García’s study.\nLiterary studies must also enter the discussion. In “I Don’t Understand Those Who Have Turned Away From Me,” Chrystos, a feminist writer and activist active throughout the 80s and 90s, describes their experience living around San Francisco and watching Stanford educated white women enjoy a decadent life from intergenerational welfare while a Black man toils away on the docks, his work destroying his body while not paying off his subpar house in his lifetime. Then Chrystos writes, “I read the funniest line in a health book yesterday. It said, that for some ‘unknown’ reason, more black people had hypertension than white people. Not funny. No mystery” (Chrystos 65). While Chrystos backs up their story with no sources or statistics, its dryness towards the subject captures an angle other disciplines miss. To really stir the pot, Chrystos adds a cynical note, questioning why this disparity is shrouded in mystery when the American history of Black oppression and a racial caste system is not a mystery. In aggregate, Black Americans have less family wealth, only a handful of generations away from slavery, enrolling in college at lower rates, and working in manual labor sectors at higher rates. Chrystos’s anecdote pairs with Stahl’s commentary: even if the Black dock worker’s son from their story graduates from Stanford like the white women, he will still have a higher risk of hypertension because he climbed the socioeconomic ladder, feeling the weight of his privilege just as Villarosa feels the social pressure to be a strong Black woman in her joints and muscles.\nWhile medical scholarship searches for more effective ways to treat ailments and improve public health, without actively engaging with extramedical considerations, like those presented in Linda Villarosa’s “Everything I Thought was Wrong,” they will not be able to fully account for the racial health disparity of hypertension rates in Black Americans. If doctors and medical scholars alike can humble themselves and listen to voices like Villarosa, they can treat Black patients’ hypertension without blaming them for their condition, like research surrounding poor medication adherence, while treating their patients like humans, unlike Villarosa’s painful story about her father’s hospitalization, and stop enabling racism within the medical field, like publishing a conclusion that disregards a large body of Afro-Cuban scholarship since the 1960s.\n\n\n\n\nReferences\n\nBenson, Devyn Spence. “\"NOT BLACKS, BUT CITIZENS\".” World Policy Journal, vol. 33, no. 1, 2016, pp. 23–29, https://www.jstor.org/stable/26781377.\n\n\nChrystos. “I Don’t Understand Those Who Have Turned Away from Me.” This Bridge Called My Back: Writings by Radical Women of Color, edited by Cherrie Moraga and Gloria Anzaldua, Persephone Press, 1981, pp. 65–67.\n\n\nFerdinand, Keith C., et al. “Disparities in Hypertension and Cardiovascular Disease in Blacks: The Critical Role of Medication Adherence.” The Journal of Clinical Hypertension, vol. 19, no. 10, 2017, pp. 1015–24, https://doi.org/https://doi.org/10.1111/jch.13089.\n\n\nLorde, Audre. “The Master’s Tools Will Never Dismantle the Master’s House.” This Bridge Called My Back: Writings by Radical Women of Color, edited by Cherrie Moraga and Gloria Anzaldua, Persephone Press, 1981, pp. 94–97.\n\n\nMorenoff, Jeffrey D., et al. “Understanding Social Disparities in Hypertension Prevalence, Awareness, Treatment, and Control: The Role of Neighborhood Context.” Placing Health in Context, vol. 65, no. 9, Nov. 2007, pp. 1853–66, https://doi.org/10.1016/j.socscimed.2007.05.038.\n\n\nPark, C., et al. “PCV91 FACTORS ASSOCIATED WITH AVOIDING HEALTHCARE AMONG ADULTS WITH HYPERTENSION: AN ANALYSIS OF THE 2015 MEDICARE CURRENT BENEFICIARY SURVEY.” ISPOR 2019: Rapid. Disruptive. Innovative: A New Era in HEOR, vol. 22, May 2019, p. S135, https://doi.org/10.1016/j.jval.2019.04.527.\n\n\nPark, Chanhyun, et al. “Cost-Effectiveness Analyses of Antihypertensive Medicines: A Systematic Review.” American Journal of Preventive Medicine, vol. 53, no. 6S2, Dec. 2017, pp. S131–42, https://doi.org/10.1016/j.amepre.2017.06.020.\n\n\nPickering, T. G. “Why Is Hypertension More Common in African Americans?” Journal of Clinical Hypertension (Greenwich, Conn.), vol. 3, no. 1, Feb. 2001, pp. 50–52, https://doi.org/10.1111/j.1524-6175.2001.990833.x.\n\n\nStahl, Sidney M. SOCIOLOGICAL FACTORS IN HIGH BLOOD PRESSURE. 1976, https://api.semanticscholar.org/CorpusID:67919008.\n\n\nVillarosa, Linda. Under the Skin: Racism, Inequality, and the Health of a Nation. First edition, Doubleday, 2022."
  },
  {
    "objectID": "writing/heat/index.html",
    "href": "writing/heat/index.html",
    "title": "Philadelphia Needs to Take Heat Seriously",
    "section": "",
    "text": "This was originally an op-ed project for my ENGL110 course “Family, Race, and Kinship”, but it was later modified more and posted on my substack before its removal from substack and posting on my personal site.\n\nLast October I saw Alex G in concert in Philadelphia at Union Transfer, a smaller venue in Philly’s Callowhill neighborhood. Callowhill sits at a uniquely liminal position in the Philadelphia neighborhood puzzle–just north of Chinatown and the Vine Street Expressway, and just south of the Richard Allen Homes project, a majority Black, affordable housing community. On a broader scale, Callowhill finds itself in the awkwardness that is Philadelphia between Center City and North Philadelphia. In a different life the neighborhood bustled with blue collar worker filled factories. Factory buildings transformed into music venues like Union Transfer and Franklin Music Hall and hip loft-style housing, and more affluent tenants snuffed out the blue collar population.\nUnion Transfer was a fantastic place to hear my favorite tracks like ‘Hope,’ a song about Alex’s formative young adult experiences in Philadelphia, and ‘Snot,’ my personal favorite. Filled to the brim with concertgoers, the venue got toasty, but I only broke a sweat on the walk to get some pizza afterwards. The jet black roads, cracked cement sidewalks, and weathered brick buildings soaked up heat all day, and the heat lingered long after the sunset. This unusual warmness of the city, formally known as the ‘heat island’ effect, is an issue for all cities, but especially for cities with too few trees.\n\n\n\nA street in Callowhill\n\n\nOn top of filling streets with their big green beauty, trees also manage how much of the sun’s rays the ground can absorb, and cool down their soil through evapotranspiration. All this fights off the heat island effect. Trees are even more effective than other ground coverings like bushes or grass. The continued worsening of the heat island effect increases the need for more trees in urban areas.\nThe issue is poor neighborhoods have less trees than wealthy neighborhoods. Formerly redlined areas–areas that received severely limited real estate investment because the government allowed banks to consider lending to Black people ‘risky’—have fewer environmental amenities to help clean and cool the air, like trees. The city of Philadelphia’s own interactive report on over 100,000 trees visually highlights the disparity. Trees evenly dot Fairmount and other uber wealthy neighborhoods, but run thin just north in Strawberry Mansion where the average household income is over $80,000 lower. And while the distribution is inequitable when considering economic class, trees in Philadelphia also stratify along lines of race—Strawberry Mansion is 98% Black.\n\n\n\nDistribution of asthma cases in North Philly (darker means higher)\n\n\n\n\n\nPercent of population living in poverty (darker means higher)\n\n\nOf course, tree disparity matters when talking about defending a community from the heat island effect and shielding sidewalks from harsh sunlight, but it mixes with poor neighborhoods having less access to air conditioning, higher rates of asthma, and higher rates of diabetes to make an unjust cocktail. So while rich neighborhoods walk tree-protected streets, the sun shines harder on less fortunate neighborhoods.\nAnd the sun will only shine harder–over the past 60 years heat waves’ intensity, duration, and frequency grew. This means hotter temperatures for longer and more often, a damning future for low income homes. Though the present is damning as well considering increasing tree cover to 30% could save 400, mostly low-income, Philadelphians from dying prematurely each year. Heat’s deadliness extends outside of cities killing more people than hurricanes, tornadoes, and lightning, and with overall rising temperatures–July 2023 being the hottest month in human history–the death toll of heat will only rise.\nTo address this issue Philadelphia needs more minds like Erica Fichman, current manager of TreePhilly. TreePhilly offers free street tree installation, grants to host tree planting community events, and tree maintenance for up to a year after planting. These grants for community tree planting events speak to the community building nature of planting trees–creating a sense of place where homeowners walk home and see trees they remember planting.\nLuckily Philadelphia recently received 12 million dollars in USDA funding from a massive government grant focused on increasing urban greenery in an equitable fashion. When spending their once in a lifetime sum of money, Philly needs to acknowledge the complex reasons low-income renters and homeowners aren’t already requesting street and yard trees. These include uncertainty around who will care for the tree, who will spend hours dialing every arborist in the phonebook until one finally agrees to prune the tree for thousands of dollars, and who will be responsible for paying to repair sidewalk damage from the roots in 5 years. And Philly needs to prepare a response for when homeowners rightfully ask if their homeownership will be protected from tax hikes from property value increases. This means hiring arborists to prune and care for planted trees, offering sidewalk repair grants, and building policy to protect freshly planted areas from tax hike driven gentrification.\nBy recognizing and actively addressing systemic barriers to tree access and maintenance, Philadelphia can forge a path toward more equitable urban development and effective use of new USDA funds."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "(insert bio here)"
  },
  {
    "objectID": "projects/304 honors/index.html",
    "href": "projects/304 honors/index.html",
    "title": "The Best Seat at UD",
    "section": "",
    "text": "This was a project for the honors section of UD’s CHE304 “Random Variability in Chemical Processes.” My main contributions to the report were the literature review, writing the introduction and discussion, and all visualization and formatting. The full report on this study is available below."
  },
  {
    "objectID": "projects/304 honors/index.html#descriptive-statistics",
    "href": "projects/304 honors/index.html#descriptive-statistics",
    "title": "The Best Seat at UD",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis section may feel like figure spam. Figure 1 contains a histogram of the measured desk areas. This shows how the entire population of desk areas is distributed. Most desks are around 200 in\\(^2\\) with very few above 600 in\\(^2\\). The super outlier is Gore 316.\n\n\n\n\n\n\nFigure 1: Histogram of Desk Area (in\\(^2\\))\n\n\n\nSince the vast majority of rooms we measured were labeled by UD’s course inventory, we can consider the different classes of rooms^[Not to be confused with classrooms.} (Table 2) as different populations. The areas of these different populations are visualized as box plots in Figure 2.\n\n\n\n\n\n\nFigure 2: Box Plots of different CIC labels\n\n\n\nThe different styles of rooms have quite different desk areas! Perhaps the auditorium desks are there for a bare minimum, multi-purpose room and the project based learning and case study rooms lend themselves to more desk space for cracking open some cases or problem sets. This is complete conjecture.\nNow we shift focus from desk area towards seat and desk height from the floor. Figure 3 shows the relationship between how high the seat is from the ground and how high the desk surface is from the seat for the different room classes.\n\n\n\n\n\n\nFigure 3: Seat Height vs. Distance Between Desk and Seat by room label\n\n\n\nFigure 4 is a visually busy yet interesting representation of the desk width and length. The red and purple rectangles are values from the literature and clearly a large subset of our data has a smaller width and length than [1]. Not a single desk had a width as thin as [2] which raises questions about what is going on with [2]. Another interesting thing is you can kinda see that the majority of desks will be around that 200 in\\(^2\\) sweet spot in Figure 1 since that 14-17 in range is where a square would have an area of 200 in\\(^2\\) or so.\n\n\n\n\n\n\nFigure 4: Length and Width measurements as Rectangles\n\n\n\nFigure 5 contains information similar to the box plot, but gives more information about the shapes of the desks for the different labels. The auditorium still shows up as an odd one out—a singleton standing lonely outside the rest of the data. Sad sad singleton.\n\n\n\n\n\n\nFigure 5: Desk Length vs. Desk Width, Categorized"
  },
  {
    "objectID": "projects/304 honors/index.html#the-best-seat",
    "href": "projects/304 honors/index.html#the-best-seat",
    "title": "The Best Seat at UD",
    "section": "The Best Seat",
    "text": "The Best Seat\nWhen calculating the seat whose parameters are closest to the literature provided parameters, we need to borrow a wrench from the ml toolbox and account for the difference in spread and magnitude of the different factors. For instance, if we just sloppily found the 2-norm distance between all of our measurements and the literature, the parameters with the largest variance and range would dominate our distance calculation. So we standardized our data by effectively converting all our measurements into t-scores using our sample mean and sample standard deviations for each feature. Equation 1 contains vague information about this standardization in math text therefore it is valid.\n\\[ x \\longrightarrow \\frac{x - \\bar{x}}{s_x}  \\tag{1} \\]\nA brief, nerdy ramble about distance surely fits in here. Many machine learning models and techniques use distance in one way or another. For \\(k\\) nearest neighbors you only know if a neighbor is near if you know how far they are. In fact, we are really searching for the 1-nearest neighbor to our references so it is quite reasonable to treat this as such and standardize the columns. Some playing around could be done with which norm should be used (2-norm, Manhattannorm, \\(\\infty\\)-nom, etc) but no part of this analysis requires something that spicy.\nNow our standardized distance should be able to catch a more accurate best desk instead of being dominated by the parameter with the highest magnitude. Using the chair height, desk height, desk length, and desk width from [2], Colburn 366 is found to be the best seat at UD. Using [1], Penny 209 is found to be the best desk at UD."
  },
  {
    "objectID": "projects/304 honors/index.html#the-worst-seat",
    "href": "projects/304 honors/index.html#the-worst-seat",
    "title": "The Best Seat at UD",
    "section": "The Worst Seat",
    "text": "The Worst Seat\nUsing the same distance metric as discussed above we may also find the worst desks with respect to both references. Compared to [2], Memorial 127 was the worst. Compared to [1] Memorial 48 and Gore 316 tied for worst desk."
  },
  {
    "objectID": "projects/304 honors/index.html#footnotes",
    "href": "projects/304 honors/index.html#footnotes",
    "title": "The Best Seat at UD",
    "section": "Footnotes",
    "text": "Footnotes\n\n\noddly [1] argues chairs with desks connected are favorable to desks where the chair is independent since students tend to lean over the desk while writing instead of keeping their back straight. But for some reason they only mention they made observations and are like “oh yeah they slouched real bad” without ever describing their methods or fully presenting their results…↩︎\nthe dimensions for both references were converted into inches with the appropriate conversion factors before their inclusion in Table 1.↩︎\nThis is in reference to the hit film ‘Morbius’ directed by Daniel Espinosa.↩︎"
  },
  {
    "objectID": "projects/harrpotter/index.html",
    "href": "projects/harrpotter/index.html",
    "title": "Did JK Rowling’s Writing Change When She Switched Her Genre?",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport nltk\n\nharry = pd.read_csv('harry_potter_books.csv')\nharry.head()\n\n\n\n\n\n\n\n\ntext\nbook\nchapter\n\n\n\n\n0\nTHE BOY WHO LIVED Mr. and Mrs. Dursley, of nu...\nBook 1: Philosopher's Stone\nchap-1\n\n\n1\nfour, Privet Drive, were proud to say that the...\nBook 1: Philosopher's Stone\nchap-1\n\n\n2\nthank you very much. They were the last people...\nBook 1: Philosopher's Stone\nchap-1\n\n\n3\nbe involved in anything strange or mysterious,...\nBook 1: Philosopher's Stone\nchap-1\n\n\n4\nwith such nonsense. Mr. Dursley was the direc...\nBook 1: Philosopher's Stone\nchap-1\n\n\n\n\n\n\n\nEach row contains some text (probably just the literal lines from whatever ebook this guy scraped the text from) and also notes the book it is from and what chapter the text is from as well. Let’s make a new column that is the number of words in each row using a very, very tasty library called NLTK.\n\nharry['word count'] = harry['text'].apply(lambda x: len(nltk.word_tokenize(x)))\nharry['word count'].head()\n\n0    11\n1    15\n2    14\n3    14\n4    12\nName: word count, dtype: int64\n\n\nTo get the clustering party started I’ll first merge every 4 rows together so that there is more info per row (and since there’s so many rows this will not hurt the sample size too much).\nThis is not trivial! I will group them into individual chapters and then re-split them. I’ll also only take chapters 3 and 4 to save on training time.\n\ncombined_rows = harry.groupby(['book', 'chapter']).agg({'text': ' '.join, 'word count': 'sum'}).reset_index()\ncombined_rows = combined_rows.loc[(combined_rows['book'] == 'Book 3: Prisoner of Azkaban') | (combined_rows['book'] == 'Book 4: Goblet of Fire')]\ncombined_rows.head()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\nword count\n\n\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n4349\n\n\n37\nBook 3: Prisoner of Azkaban\nchap-10\nTHE MARAUDER'S MAP Madam Pomfrey insisted o...\n8973\n\n\n38\nBook 3: Prisoner of Azkaban\nchap-11\nTHE FIREBOLT Harry didn't have a very clear...\n6553\n\n\n39\nBook 3: Prisoner of Azkaban\nchap-12\nTHE PATRONUS Harry knew that Hermione had m...\n6230\n\n\n40\nBook 3: Prisoner of Azkaban\nchap-13\nGRYFFINDOR VERSUS RAVENCLAW It looked like ...\n5398\n\n\n\n\n\n\n\nSeeing how many words are in each row its clear we have a row per chapter. Now to let NLTK do the sentence splitting magic and EXPLODE the rows into their own rows.\n\ncombined_rows['text'] = combined_rows['text'].apply(lambda x: nltk.sent_tokenize(x))\nsentences = combined_rows.explode('text').drop('word count', axis=1)\nsentences.head()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\n\n\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nFor one thing, he hated the summer holidays mo...\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nFor another, he really wanted to do his homewo...\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nAnd he also happened to be a wizard.\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nIt was nearly midnight, and he was lying on hi...\n\n\n\n\n\n\n\n\nsentences.describe()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\n\n\n\n\ncount\n20630\n20630\n20630\n\n\nunique\n2\n37\n18399\n\n\ntop\nBook 4: Goblet of Fire\nchap-9\n.\n\n\nfreq\n14010\n905\n2048\n\n\n\n\n\n\n\nIt’s a little annoying that NLTK thinks a solid number of sentences are a single period so I’ll remove all those rows just in case they ruin my clustering later. (there are some other annoyingly short sentences I removed too)\n\nsentences = sentences.loc[(sentences['text'] != '.') & (sentences['text'] != \".'\") & (sentences['text'] != \"...\")]\nsentences.describe()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\n\n\n\n\ncount\n18544\n18544\n18544\n\n\nunique\n2\n37\n18396\n\n\ntop\nBook 4: Goblet of Fire\nchap-21\nClunk.\n\n\nfreq\n11925\n851\n9\n\n\n\n\n\n\n\nNow I’ll use some useful tools from NLTK to make columns to use when clustering\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\nsentences['word_count'] = sentences['text'].apply(lambda x: len(word_tokenize(x)))\nsentences['avg_word_length'] = sentences['text'].apply(lambda x: sum(len(word) for word in word_tokenize(x)) / len(word_tokenize(x)))\nsentences['stopwords_count'] = sentences['text'].apply(lambda x: len([word for word in word_tokenize(x) if word.lower() in stop_words]))\nsentences['sentence_length'] = sentences['text'].apply(lambda x: len(x.split()))\n\nimport string\nsentences['punctuation_count'] = sentences['text'].apply(lambda x: len([char for char in x if char in string.punctuation]))\n\n\nsentences.head(1)\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\nword_count\navg_word_length\nstopwords_count\nsentence_length\npunctuation_count\n\n\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n13\n3.769231\n3\n12\n1\n\n\n\n\n\n\n\nAdding a few more columns that also make use of NLTKs cooler features like polarity.\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n# sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\nsentences['polarity'] = sentences['text'].apply(lambda x: sid.polarity_scores(x)['compound'] )\nsentences['subjectivity_score'] = sentences['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\nfor tag in ['JJ', 'RB', 'NN', 'VB']:\n    sentences[tag + '_count'] = sentences['text'].apply(lambda x: sum(1 for _, pos in nltk.pos_tag(nltk.word_tokenize(x)) if pos == tag))\n\nThe tags ‘JJ’, ‘RB’, ‘NN’, and ‘VB’ correspond to parts of speech adjective, adverb, noun, and verb.\n\nsentences.reset_index(drop=True, inplace=True)\nsentences.head(5)\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\nword_count\navg_word_length\nstopwords_count\nsentence_length\npunctuation_count\npolarity\nsubjectivity_score\nJJ_count\nRB_count\nNN_count\nVB_count\n\n\n\n\n0\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n13\n3.769231\n3\n12\n1\n0.0000\n0.0000\n2\n1\n1\n0\n\n\n1\nBook 3: Prisoner of Azkaban\nchap-1\nFor one thing, he hated the summer holidays mo...\n17\n3.705882\n8\n15\n2\n-0.3818\n-0.3818\n1\n0\n4\n0\n\n\n2\nBook 3: Prisoner of Azkaban\nchap-1\nFor another, he really wanted to do his homewo...\n25\n3.360000\n14\n22\n3\n-0.8990\n-0.8990\n1\n1\n3\n2\n\n\n3\nBook 3: Prisoner of Azkaban\nchap-1\nAnd he also happened to be a wizard.\n9\n3.222222\n5\n8\n1\n0.0000\n0.0000\n0\n1\n1\n1\n\n\n4\nBook 3: Prisoner of Azkaban\nchap-1\nIt was nearly midnight, and he was lying on hi...\n51\n3.941176\n21\n45\n7\n0.3182\n0.3182\n3\n2\n10\n0\n\n\n\n\n\n\n\n\nAnalysis / Results\nI’ll use standarized colums and consider \\(n \\in (2,3,4,5,6,7)\\). I’ll also use standard state 51524 because thats todays date if you were wondering.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nfrom sklearn.metrics import silhouette_samples\n\nX = sentences.drop(['book', 'chapter', 'text' ], axis=1)\n\n\nks = [2,3,4,5,6,7]\n\nfor k in ks:\n    km = make_pipeline(StandardScaler(), KMeans(n_clusters=k, n_init=5, random_state=51524))\n    km.fit(X)\n    y = km[1].labels_\n    sil = silhouette_samples(X, y)\n    ARI = adjusted_rand_score(sentences['book'], y)\n    print(f'k={k}, silhouette={sil.mean()}, ARI={ARI:.4f}')\n\nk=2, silhouette=0.5636526958226444, ARI=-0.0020\nk=3, silhouette=0.4692951542966001, ARI=-0.0016\nk=4, silhouette=0.2630215379212679, ARI=0.0005\nk=5, silhouette=0.034977983880398036, ARI=-0.0003\nk=6, silhouette=0.010108118950209011, ARI=-0.0005\nk=7, silhouette=-0.005114249138976739, ARI=0.0002\n\n\n2 is looking like the most delicious number of clusters and also matches up with my question so I’ll continue on with \\(n=2\\) and visualize its box plot.\n\nkm = make_pipeline(StandardScaler(), KMeans(n_clusters=2, n_init=10, random_state=51524))\nkm.fit(X)\n\ny = km[1].labels_\nsentences[\"cluster\"] = y\nsentences[\"sil\"] = silhouette_samples(X, y)\nsns.catplot(data=sentences, x=\"cluster\", y=\"sil\", kind=\"box\")\n\n\n\n\n\n\n\n\nLooks like cluster 0 is decent except for the crazy amount of outliers and cluster 1 is pretty bad.\nGoing forward with the agglomerative clustering, I’ll only look at \\(n = {2,3,4}\\) since for k-means the other n values were trash and this will save lots of computing time. I’ll also standardize X myself so I don’t need to mess around with pipelines and not even bother with single-linkage because for all the values of \\(n\\) I tested it made \\(n-1\\) singletons.\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nfor col in X.columns:\n    X[col] = StandardScaler().fit_transform(X[[col]])\n\n\nlinkages = [\"ward\", \"complete\", \"average\"]\n\nresults = pd.DataFrame()\n\nfor linkage in linkages:\n    data = X.copy()\n    agg = AgglomerativeClustering(n_clusters=2, linkage=linkage)\n    agg.fit(X)\n    data[\"cluster\"] = agg.labels_\n    data[\"sil\"] = silhouette_samples(X, y)\n    data[\"linkage\"] = linkage\n    results = pd.concat( (results, data) )\nsns.catplot(data=results, x=\"cluster\", y=\"sil\", col='linkage', kind=\"box\")\n\nfor linkage in linkages:\n    data = X.copy()\n    agg = AgglomerativeClustering(n_clusters=3, linkage=linkage)\n    agg.fit(X)\n    data[\"cluster\"] = agg.labels_\n    data[\"sil\"] = silhouette_samples(X, y)\n    data[\"linkage\"] = linkage\n    results = pd.concat( (results, data) )\nsns.catplot(data=results, x=\"cluster\", y=\"sil\", col='linkage', kind=\"box\")\n\nfor linkage in linkages:\n    data = X.copy()\n    agg = AgglomerativeClustering(n_clusters=4, linkage=linkage)\n    agg.fit(X)\n    data[\"cluster\"] = agg.labels_\n    data[\"sil\"] = silhouette_samples(X, y)\n    data[\"linkage\"] = linkage\n    results = pd.concat( (results, data) )\nsns.catplot(data=results, x=\"cluster\", y=\"sil\", col='linkage', kind=\"box\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaking a peek at the ARI\n\nn = [2,3,4]\n\nfor n_clusters in n:\n    for linkage in linkages:\n        agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n        y = agg.fit_predict(X)\n        ARI = adjusted_rand_score(sentences['book'], y)\n        print(f\"ARI for {linkage} linkage with {n_clusters} clusters is {ARI:.4f}\")\n\nARI for ward linkage with 2 clusters is -0.0008\nARI for complete linkage with 2 clusters is 0.0002\nARI for average linkage with 2 clusters is 0.0001\nARI for ward linkage with 3 clusters is -0.0012\nARI for complete linkage with 3 clusters is 0.0002\nARI for average linkage with 3 clusters is 0.0011\nARI for ward linkage with 4 clusters is -0.0016\nARI for complete linkage with 4 clusters is 0.0003\nARI for average linkage with 4 clusters is 0.0012\n\n\nThese ARI scores are pretty bad from a classification standpoint, but it may indicate that Rowling’s writing didn’t truly change from book 3 to 4.\nAs a final MATH219 effort I’ll choose only the top 25% of sentences and do similar speed-run analysis on them since maybe the shorter sentences lack writing style.\n\nX = sentences.drop(['book', 'chapter', 'text'], axis=1).loc[sentences['word_count'] &gt; 26]\nX = StandardScaler().fit_transform(X)\n\nAgglomerative:\n\nn = [2,3,4]\nlinkages = [\"average\", \"single\", \"ward\"]\n\nfor n_clusters in n:\n    for linkage in linkages:\n        agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n        y = agg.fit_predict(X)\n        ARI = adjusted_rand_score(sentences['book'].loc[sentences['word_count'] &gt; 26], y)\n        print(f\"ARI for {linkage} linkage with {n_clusters} clusters is {ARI:.4f}\")\n\nARI for average linkage with 2 clusters is 0.0004\nARI for single linkage with 2 clusters is 0.0004\nARI for ward linkage with 2 clusters is 0.0042\nARI for average linkage with 3 clusters is 0.0008\nARI for single linkage with 3 clusters is 0.0008\nARI for ward linkage with 3 clusters is 0.0012\nARI for average linkage with 4 clusters is 0.0008\nARI for single linkage with 4 clusters is 0.0004\nARI for ward linkage with 4 clusters is 0.0005\n\n\nK-means:\n\nfor n in [2]:\n    km = make_pipeline(StandardScaler(), KMeans(n_clusters=2, n_init=10, random_state=51524))\n    km.fit(X)\n    ARI = adjusted_rand_score(sentences['book'].loc[sentences['word_count'] &gt; 26], km[1].labels_)\n    print(f\"ARI for KMeans with {n} clusters is {ARI:.4f}\")\n\nARI for KMeans with 2 clusters is -0.0070\n\n\n\n\nDiscussion\nThis project originally set out to see if Rowling’s writing on the sentence scale truly changed when she switched her genre from middle grade (books 1-3) to young adult (books 4-7, obviously). Lloyd’s algorithm and a handful of linkage types for agglomerative clustering couldn’t seem to get clusters anywhere near accurate to what book the rows (sentences) came from.\nI originally thought ‘hey maybe its just annoying short sentences kicking around,’ but looking at the top 25% longest sentences revealed all agglomerative clustering linkages couldn’t find the secret (resisting the urge to write jokes about magic) and poor old lloyd (k-means) couldn’t either.\nThe only reasonable takeaway is that on the sentence scale Rowling’s writing did not change in a NLTK implementable, measurable way. Further analysis would need to be done to determine if on a larger scale (maybe paragraph or page) her writing did change to better fit the young adult tag."
  },
  {
    "objectID": "projects/pfd maker/index.html",
    "href": "projects/pfd maker/index.html",
    "title": "Process Flow Diagram Maker",
    "section": "",
    "text": "Process Flow Diagrams (PFDs) are a useful tool in visualizing chemical engineering processes, but making them with tikzpictures is very annoying and slow. But if you use tikzpictures/latex the diagram will be rendered as a vector graphic so you can zoom in without losing image quality. This project works at making them as easily as you’d edit a google slide show or something.\nHere are some examples of PDFs I made with the help of my gui (the whole document is here)\n   \na writeup about the controls and the python file for this project can be found here"
  },
  {
    "objectID": "projects/proj.html",
    "href": "projects/proj.html",
    "title": "Projects",
    "section": "",
    "text": "CHEG304 Python\n\n\nProviding a resource for doing CHEG304 coursework with python instead of matlab\n\n\n\nstatistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Best Seat at UD\n\n\nAn observational study on the chairs and desks around UD’s campus\n\n\n\nstatistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcess Flow Diagram Maker\n\n\na small tkinter program to draw process flow diagrams and generate TikzPicture code\n\n\n\nlatex\n\n\ngui\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDid JK Rowling’s Writing Change When She Switched Her Genre?\n\n\nUsing NLTK to create features and common clustering algorithms on Harry Potter text\n\n\n\nclustering\n\n\nmath219\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow does Philadelphia weather affect bails?\n\n\nUsing data from the city of Philadelphia about bails and weather to invesigate their relationship with common classification and regression algorithms.\n\n\n\nclassification\n\n\nregression\n\n\nmath219\n\n\npython\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/304/index.html",
    "href": "projects/304/index.html",
    "title": "CHEG304 Python",
    "section": "",
    "text": "Every UD chemical engineering student takes CHEG304 “Random Variability in Chemical Processes”. The professor is lovely, but they were taught matlab and therefore share helpful matlab functions. Over a week I put together this pamphlet citing the functions our professor shared and then pointing to python functions that are functionally similar if not the exact same. I also included tons of examples of how to use some of the functions along with plenty of helpful warnings and notes.\nThe book is free to access here"
  },
  {
    "objectID": "projects/weatherbails/index.html",
    "href": "projects/weatherbails/index.html",
    "title": "How does Philadelphia weather affect bails?",
    "section": "",
    "text": "Introduction\nData sourced from the city of Philadelphia. This is a data set that includes information about what bails were set and for what crime over the entire city of Philadelphia from 2014 to 2022. Data about daily weather in Philadelphia (specifically measured at the Philadelphia Airport) sourced from National Centers for Environmental Information. The formal documentation lists out what each column in the weather dataset means, so I’ll leave the column names as they are to not stray from the documentation.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nbails = pd.read_csv('bail_data_daily_citywide.txt')\nweather = pd.read_csv('daily_weather_data.csv')\n\nI’ll merge the sets in the same manner as project 1, and fill NaN values with 0 as none of the values important to this analysis will be affected by filling NaN with 0.\n\ntotals = bails[bails['bail_category'] == 'Total']\nbails = bails[bails['bail_category'] != 'Total']\n\n#saving the list of columns for later !!\nbail_columns = bails.columns[2:]\n\nbails = bails.merge(weather, left_on='date_value', right_on='DATE', how='left')\n\n#getting rid of some useless columns\nbails.drop(columns=['DATE', 'STATION', 'NAME'], inplace=True)\nbails = bails.fillna(0)\n\nprint(bails.columns)\n\nIndex(['date_value', 'bail_category', 'Aggravated Assault',\n       'Altered Firearm Serial Number', 'Arson', 'Attempted Murder',\n       'Auto Theft', 'Burglary/Commercial', 'Burglary/Residential',\n       'Carjacking', 'Criminal Mischief', 'Disorderly Conduct',\n       'Drug Possession', 'Drug Possession in Jails', 'Drug Sales',\n       'Drug Sales with a Firearm', 'DUI', 'Ethnic Intimidation',\n       'Firearm Possession by a Prohibited Person',\n       'Firearm Possession without a License', 'Fraud', 'Homicide: Other',\n       'Homicide: Shooting', 'Illegal Dumping/Littering', 'Non-Fatal Shooting',\n       'Other Assaults', 'Other Firearm Offenses', 'Other Property Crimes',\n       'Other Violent Crimes', 'Patronizing Prostitutes/Sex Workers',\n       'Promoting Prostitution', 'Prostitution/Sex Work', 'Rape',\n       'Retail Theft', 'Robbery', 'Robbery with a Deadly Weapon',\n       'Sexual Assault and Other Sex Offenses', 'Simple Assault',\n       'Strangulation', 'Theft', 'Theft from Auto', 'Threats of Violence',\n       'Trespass', 'Uncategorized Offenses',\n       'Victim/Witness Intimidation & Retaliation',\n       'Violation of Protection Order', 'AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW',\n       'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5',\n       'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08', 'WT09'],\n      dtype='object')\n\n\nI’ll also quick just describe the weather columns to get an idea of what they’re looking like\n\nbails[['AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW',\n       'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5',\n       'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08', 'WT09']].describe()\n\n\n\n\n\n\n\n\nAWND\nPGTM\nPRCP\nPSUN\nSNOW\nSNWD\nTAVG\nTMAX\nTMIN\nTSUN\n...\nWSF2\nWSF5\nWT01\nWT02\nWT03\nWT04\nWT05\nWT06\nWT08\nWT09\n\n\n\n\ncount\n24256.000000\n24256.000000\n24256.000000\n24256.0\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.0\n...\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n\n\nmean\n9.039241\n0.201847\n0.127503\n0.0\n0.069360\n0.151880\n54.852243\n65.370712\n48.519789\n0.0\n...\n19.786972\n26.166260\n0.344987\n0.026715\n0.080805\n0.013852\n0.000989\n0.009894\n0.094327\n0.002309\n\n\nstd\n3.706535\n11.112817\n0.359605\n0.0\n0.636483\n0.968468\n21.252877\n18.588487\n17.187521\n0.0\n...\n6.645629\n8.328464\n0.475374\n0.161253\n0.272541\n0.116880\n0.031441\n0.098980\n0.292289\n0.047994\n\n\nmin\n0.670000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n0.000000\n13.000000\n2.000000\n0.0\n...\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n6.490000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n42.000000\n50.000000\n34.000000\n0.0\n...\n15.000000\n19.900000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n8.500000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n58.000000\n67.000000\n48.000000\n0.0\n...\n18.100000\n25.100000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n10.960000\n0.000000\n0.060000\n0.0\n0.000000\n0.000000\n73.000000\n82.000000\n64.000000\n0.0\n...\n23.000000\n30.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26.620000\n612.000000\n4.760000\n0.0\n19.400000\n18.100000\n90.000000\n98.000000\n82.000000\n0.0\n...\n55.000000\n72.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 22 columns\n\n\n\nNow, I’ll make some pairplots to visualize the relationships between ethnic intimidation and all the weather data. This will come in handy when I do some data hengineering\n(this part removed for github upload)\n\n\nClassification\nI will try and answer this question:\nCan data about the average wind speed, peak gust time, precipitations, daily percent of possible sunshine, snowfall, snow depth, average temperature, max temperature, min temperature, total minutes of sunshine, direction of fastest wind for 2 and 5 minute periods, fastest wind speed in 2 and 5 minutes, and weather types (classes) predict whether 2 or more ethnic intimidation bails were posted for $25-$100k?\nI’ll choose recall as my metric because recall is effectively ‘out of all the true cases what proportion did it identify,’ and in the context of this question it makes a lot of sense to gague the learner’s ability based on correctly identifying the true cases and ‘punish’ the learner for false negatives.\nNow, I’ll setup my \\(\\textbf{X}\\) and \\(\\textbf{y}\\) and split them into training and testing sets with random state of 4202024 because today is 4/20/2024.\n\nX = bails.loc[(bails['bail_category'] == '$25k-$100k') & (bails['Drug Sales'] &gt; 0)][['AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW',\n       'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5',\n       'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08', 'WT09']]\ny = bails.loc[(bails['bail_category'] == '$25k-$100k') & (bails['Drug Sales'] &gt; 0)]['Drug Sales'] &gt; 1\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True,random_state=4202024)\n\nNow I’ll go through a grid search over a handful of values for max depth and minimum samples per leaf on some decision tree classifiers.\n\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\n\ngrid = { \"max_depth\":[6,8,10,12,14], \n         \"min_samples_leaf\":[1,2,3,4,7]}\nlearner = DecisionTreeClassifier(random_state=420)\nkf = StratifiedKFold(n_splits=8, shuffle=True, random_state=2024)\n\ngrid_search = GridSearchCV(\n    learner, grid, \n    scoring=\"recall\", \n    cv=kf,\n    n_jobs=-1\n    )\ngrid_search.fit(X_train, y_train)\n\ntree_best_params = grid_search.best_params_\ntree_best_score = grid_search.best_score_\nprint(tree_best_params)\nprint(tree_best_score)\n\n{'max_depth': 8, 'min_samples_leaf': 3}\n0.6368873742291463\n\n\nThat’s depressing. Now I’ll do a similar grid search for a kNN learner with\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ngrid = {\n    'n_neighbors': range(1, 21, 2),\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan', 'minkowski'],\n    'leaf_size': list(range(1,50)),\n}\nlearner = KNeighborsClassifier()\nkf = StratifiedKFold(n_splits=8, shuffle=True, random_state=302)\n\ngrid_search = GridSearchCV(\n    learner, grid, \n    scoring=\"recall\", \n    cv=kf,\n    n_jobs=-1\n    );\ngrid_search.fit(X_train, y_train);\n\nkNN_best_params = grid_search.best_params_\nkNN_best_score = grid_search.best_score_\nprint(kNN_best_params)\nprint(kNN_best_score)\n\n{'leaf_size': 1, 'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n0.5464135021097046\n\n\nNext I’ll put together a decision tree with the best parameters from the grid search.\n\nbest_tree = DecisionTreeClassifier(**tree_best_params)\nbest_tree.fit(X_train, y_train)\n\nfeature_importances = best_tree.feature_importances_\ncolumns = X_train.columns\nfeature_importances = pd.Series(feature_importances, index=columns)\nfeature_importances = feature_importances.sort_values(ascending=True)\nprint(feature_importances)\n\nWT09    0.000000\nWT03    0.000000\nWT01    0.000000\nWT04    0.000000\nWT06    0.000000\nTSUN    0.000000\nSNOW    0.000000\nPSUN    0.000000\nPGTM    0.000000\nWT05    0.000000\nWT02    0.011778\nSNWD    0.015576\nWT08    0.020982\nPRCP    0.031532\nWSF2    0.057685\nWDF5    0.059225\nTMAX    0.083542\nWSF5    0.093361\nTAVG    0.115266\nWDF2    0.142408\nTMIN    0.168358\nAWND    0.200287\ndtype: float64\n\n\nit’s a little sad the learner doesn’t care much about the sun or snow, but it looks like there might be something going on with the temperature.\nNow, I’ll see if I can put together a whole bunch of these admittedly trash classifiers to make one mega classifier.\n\nfrom sklearn.discriminant_analysis import StandardScaler\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.pipeline import make_pipeline\n\nknn = KNeighborsClassifier(**kNN_best_params)\npipe = make_pipeline(StandardScaler(), knn)\nbag = BaggingClassifier(pipe, n_estimators=50, max_samples=0.15, oob_score=True, random_state=302)\nbag.fit(X_train, y_train)\n\nfrom sklearn.metrics import classification_report\ny_pred = bag.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n       False       0.42      0.46      0.44       127\n        True       0.60      0.55      0.58       182\n\n    accuracy                           0.52       309\n   macro avg       0.51      0.51      0.51       309\nweighted avg       0.53      0.52      0.52       309\n\n\n\nThats looking marginally better! Lets look at some validation curves. First, lets look at changing the max_features\n\nfrom sklearn.model_selection import ValidationCurveDisplay\n\nValidationCurveDisplay.from_estimator(\n    bag, X, y,\n    param_name=\"max_features\", \n    param_range=np.linspace(0, 0.5, 20),\n    cv=StratifiedKFold(n_splits=6, shuffle=True, random_state=19716),\n    scoring=\"recall\",\n    n_jobs=-1\n    )\n\n\n\n\n\n\n\n\nI only did the max_samples from 0-0.2 because realistically showing each learner 20% of the data would already make them too correlated with eachother.\nNow I’ll set the max features to 0.3 because that’s where it seems to steady out. Let’s look at max_samples\n\nbag = BaggingClassifier(pipe, n_estimators=50, max_samples=0.15,max_features=0.3, oob_score=True, random_state=302)\nbag.fit(X_train, y_train)\n\nValidationCurveDisplay.from_estimator(\n    bag, X, y,\n    param_name=\"max_samples\", \n    param_range=np.linspace(0, 0.5, 10),\n    cv=StratifiedKFold(n_splits=6, shuffle=True, random_state=19716),\n    scoring=\"recall\",\n    n_jobs=-1\n    )\n\n\n\n\n\n\n\n\nNow lets look at how the 0.1 max_samples learner performs more closely to see if it performs better than the best decision tree\n\nbag = BaggingClassifier(pipe, n_estimators=50, max_samples=0.1,max_features=0.3, oob_score=True, random_state=302)\nbag.fit(X_train, y_train)\n\nprint(classification_report(y_test, best_tree.predict(X_test)))\nprint(classification_report(y_train, bag.predict(X_train)))\n\n              precision    recall  f1-score   support\n\n       False       0.41      0.33      0.37       127\n        True       0.59      0.66      0.62       182\n\n    accuracy                           0.53       309\n   macro avg       0.50      0.50      0.49       309\nweighted avg       0.51      0.53      0.52       309\n\n              precision    recall  f1-score   support\n\n       False       0.57      0.41      0.48       606\n        True       0.55      0.71      0.62       628\n\n    accuracy                           0.56      1234\n   macro avg       0.56      0.56      0.55      1234\nweighted avg       0.56      0.56      0.55      1234\n\n\n\nIt looks like the bag is performing quite well compared to the measly best tree. Let’s look at the ROC curve for the bag\n\nfrom sklearn.metrics import roc_curve\n\nresults = []\nactual = y_test\np_hat = bag.predict_proba(X_test)\nfp, tp, theta = roc_curve(actual,p_hat[:,1])\nresults.extend( [('satisfied',fp,tp) for fp,tp in zip(fp,tp)] )\nroc = pd.DataFrame( results, columns=[\"label\",\"FP rate\",\"TP rate\"] )\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None\n    )\n\n\n\n\n\n\n\n\nWell, that’s not looking great, but it’s not looking disastrous.\n\n\nRegression\nquestion: can I create a model to predict what proportion of all bails posted were from car theft using the same weather data as the classification section (average wind speed, peak gust time, precipitations, daily percent of possible sunshine, snowfall, snow depth, average temperature, max temperature, min temperature, total minutes of sunshine, direction of fastest wind for 2 and 5 minute periods, fastest wind speed in 2 and 5 minutes, and weather types (classes)).\nI’ll start by putting together a list of the features to feed to some models with the targets\n\nprint(totals)\n\ntotals['sum_bails'] = totals[bail_columns].sum(axis=1)\n\ntotals = totals.merge(weather, left_on='date_value', right_on='DATE', how='left')\n\n       date_value bail_category  Aggravated Assault   \n0      2014-01-01         Total                  18  \\\n9      2014-01-02         Total                  38   \n18     2014-01-03         Total                  13   \n27     2014-01-04         Total                  12   \n36     2014-01-05         Total                   5   \n...           ...           ...                 ...   \n27243  2022-04-16         Total                  10   \n27252  2022-04-17         Total                   6   \n27261  2022-04-18         Total                   9   \n27270  2022-04-19         Total                  10   \n27279  2022-04-20         Total                   4   \n\n       Altered Firearm Serial Number  Arson  Attempted Murder  Auto Theft   \n0                                  0      0                 2           4  \\\n9                                  1      0                 4           3   \n18                                 0      0                 0           2   \n27                                 0      0                 0           5   \n36                                 0      0                 1           0   \n...                              ...    ...               ...         ...   \n27243                              0      2                 0           4   \n27252                              1      0                 0          11   \n27261                              0      0                 0           1   \n27270                              0      1                 0           2   \n27279                              0      0                 0           1   \n\n       Burglary/Commercial  Burglary/Residential  Carjacking  ...   \n0                        1                     3           0  ...  \\\n9                        0                     3           1  ...   \n18                       1                     1           0  ...   \n27                       0                     2           0  ...   \n36                       0                     0           0  ...   \n...                    ...                   ...         ...  ...   \n27243                    1                     2           0  ...   \n27252                    0                     2           1  ...   \n27261                    0                     2           1  ...   \n27270                    6                     2           0  ...   \n27279                    0                     1           0  ...   \n\n       Sexual Assault and Other Sex Offenses  Simple Assault  Strangulation   \n0                                          1               8              0  \\\n9                                          1               6              0   \n18                                         1               5              0   \n27                                         0               1              0   \n36                                         1               6              0   \n...                                      ...             ...            ...   \n27243                                      0               2              1   \n27252                                      0               2              1   \n27261                                      0               4              1   \n27270                                      0               2              0   \n27279                                      0               1              1   \n\n       Theft  Theft from Auto  Threats of Violence  Trespass   \n0          4                0                    1         0  \\\n9          6                0                    2         0   \n18         0                0                    2         2   \n27         6                0                    1         0   \n36         3                0                    0         0   \n...      ...              ...                  ...       ...   \n27243      3                0                    1         0   \n27252      1                0                    0         1   \n27261      0                0                    0         2   \n27270      2                0                    0         3   \n27279      2                0                    0         0   \n\n       Uncategorized Offenses  Victim/Witness Intimidation & Retaliation   \n0                           4                                          0  \\\n9                           6                                          1   \n18                          1                                          0   \n27                          5                                          0   \n36                          2                                          0   \n...                       ...                                        ...   \n27243                       7                                          0   \n27252                       0                                          0   \n27261                       2                                          0   \n27270                       1                                          0   \n27279                       2                                          0   \n\n       Violation of Protection Order  \n0                                  2  \n9                                  4  \n18                                 1  \n27                                 0  \n36                                 0  \n...                              ...  \n27243                              1  \n27252                              0  \n27261                              0  \n27270                              1  \n27279                              0  \n\n[3032 rows x 46 columns]\n\n\nNow that all that is set up, it’s time to get this data stew brewing. I’ll use \\(R^2\\) as my metric because I’m boring. Let’s jump into some ridge regression:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\n\nX = totals[['AWND', 'PGTM', 'PRCP',\n       'PSUN', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5',\n       'WSF2', 'WSF5', 'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08',\n       'WT09']].fillna(0)\ny = totals['Carjacking'] / totals['sum_bails']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True,random_state=4202024)\n\n\nrr = Ridge(alpha=0.5)\npipe = make_pipeline(StandardScaler(), rr)\npipe.fit(X_train, y_train)\nprint(f\"ridge CoD score: {pipe.score(X_test, y_test):.4f}\")\n\nridge CoD score: 0.0064\n\n\nThat score is like… terrible. To see is this is just a bad initial alpha selection I’ll graoh the CoD as a function of \\(\\alpha\\)\n\ndef test_alpha(alphaa):\n    rr = Ridge(alpha=alphaa)\n    pipe = make_pipeline(StandardScaler(), rr)\n    pipe.fit(X_train, y_train)\n    return pipe.score(X_test, y_test)\n\nalphas = np.linspace(0, 1000, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"r2 score\")\n\nbest_alpha = alphas[np.argmax(scores)]\nprint(f\"best alpha: {best_alpha:.4f}\")\nbest_score = np.max(scores)\nprint(f\"best score: {best_score:.4f}\")\n\nbest alpha: 404.0404\nbest score: 0.0101\n\n\n\n\n\n\n\n\n\nThat smooth curve is really beautiful, but all of these \\(R^2\\) scores are trash. Even though this is an absolutely terrible model, let’s look under the hood and see these feature weights\n\nrr = Ridge(alpha=best_alpha)\npipe = make_pipeline(StandardScaler(), rr)\npipe.fit(X_train, y_train)\n\nweights_ridge = pd.Series(pipe.named_steps['ridge'].coef_, index=X_test.columns)\nweights_ridge = weights_ridge.sort_values(ascending=True)\nprint(weights_ridge)\n\nWT04   -0.000299\nTMIN   -0.000293\nWDF5   -0.000225\nTMAX   -0.000204\nSNOW   -0.000188\nWT09   -0.000138\nWSF2   -0.000136\nWT03   -0.000123\nPGTM   -0.000096\nWT05   -0.000053\nSNWD   -0.000048\nWT01   -0.000014\nPSUN    0.000000\nTSUN    0.000000\nWSF5    0.000008\nWT02    0.000025\nWT08    0.000028\nPRCP    0.000136\nAWND    0.000311\nWT06    0.000337\nWDF2    0.000546\nTAVG    0.000565\ndtype: float64\n\n\nLooks like average temperature has the highest influence which might mean (admittedly it is WILD to harvest any meaning from this model) that during warmer days a higher proportion of bails posted were car jackings. This might explain why my AP stats teacher got his catalytic converter stolen early last summer.\nLet’s take a peek at some LASSO now\n\nfrom sklearn.linear_model import Lasso\nlass = Lasso(alpha=0.5)\nlass.fit(X_train, y_train)\nweights_lass = pd.Series(lass.coef_, index=X_train.columns)\nprint(weights_lass)\ndropped = weights_lass[weights_lass == 0].index\nprint('dropped cols:' + '\\n', dropped)\n\nAWND    0.0\nPGTM   -0.0\nPRCP    0.0\nPSUN    0.0\nSNOW   -0.0\nSNWD   -0.0\nTAVG    0.0\nTMAX   -0.0\nTMIN   -0.0\nTSUN    0.0\nWDF2    0.0\nWDF5    0.0\nWSF2    0.0\nWSF5    0.0\nWT01   -0.0\nWT02    0.0\nWT03   -0.0\nWT04   -0.0\nWT05   -0.0\nWT06    0.0\nWT08   -0.0\nWT09   -0.0\ndtype: float64\ndropped cols:\n Index(['AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN',\n       'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5', 'WT01', 'WT02', 'WT03', 'WT04',\n       'WT05', 'WT06', 'WT08', 'WT09'],\n      dtype='object')\n\n\nI don’t think thats supposed to look like that….. I’ll plot the sum of the absolute values of the weights as a function of alpha to see if theres any alpha value that gets some non-zero weights.\n\ndef test_alpha(alphaa):\n    lass = Lasso(alpha=alphaa)\n    lass.fit(X_train, y_train)\n    return abs(lass.coef_).sum()\n\nalphas = np.linspace(0, 10, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"sum abs(coeff_)\")\n\nText(0, 0.5, 'sum abs(coeff_)')\n\n\n\n\n\n\n\n\n\nthat’s also not looking too promising, but I’ll zoom in near zero to see what’s going on.\n\nalphas = np.linspace(0, 0.001, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"sum abs(coeff_)\")\n\nText(0, 0.5, 'sum abs(coeff_)')\n\n\n\n\n\n\n\n\n\nIt looks like the LASSO’s ability to smoosh weights down to 0 squishes everything down to 0 for even slightly non-zero \\(\\alpha\\) values\nenough with all these \\(lines\\) I’ve been hearing about. let’s take a trip back to the forest (look at some trees (do some non-linear regression with a decision tree regressor))\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ngrid = { \"max_depth\":[2,6,8,10,12,14], \n         \"min_samples_leaf\":[2,3,4,7]}\nlearner = DecisionTreeRegressor(random_state=420)\n\ngrid_search = GridSearchCV(\n    learner, grid, \n    scoring=\"r2\", \n    n_jobs=-1\n    )\ngrid_search.fit(X_train, y_train)\n\ntree_best_params = grid_search.best_params_\ntree_best_score = grid_search.best_score_\nprint(tree_best_params)\n\ntreez = DecisionTreeRegressor(**tree_best_params)\ntreez.fit(X_train, y_train)\nCofD_dtr = treez.score(X_test, y_test)\nprint(f\"Decision Tree Regressor r2 score: {CofD_dtr:.4f}\")\ntop_feature = X_train.columns[np.argmax(treez.feature_importances_)]\nprint(f\"Most important feature: {top_feature}\")\n\n{'max_depth': 2, 'min_samples_leaf': 2}\nDecision Tree Regressor r2 score: -0.0018\nMost important feature: TAVG\n\n\nThe forest failed me. As one last thing to try and get a half decent result, I’ll do a little data \\(hengineering\\) and make some new columns that are functions of other ones (non-zero entries of course).\n\naddnl_columns = []\nfor column in totals.columns:\n    if type(totals[column][0]) == float:\n        totals[f\"log_{column}\"] = np.log(totals[column])\n        totals[f\"sqrt_{column}\"] = np.sqrt(totals[column])\n        totals[f\"exp_{column}\"] = np.exp(totals[column])\n        totals[f\"inv_{column}\"] = 1 / totals[column]\n        addnl_columns.append(f\"log_{column}\")\n        addnl_columns.append(f\"sqrt_{column}\")\n        addnl_columns.append(f\"exp_{column}\")\n        addnl_columns.append(f\"inv_{column}\")\ntotals['prod_tavgmax'] = totals['TAVG'] * totals['TMAX']\n\n\nX = totals[['AWND', 'PGTM', 'PRCP',\n       'PSUN', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5',\n       'WSF2', 'WSF5', 'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08',\n       'WT09'] + addnl_columns].fillna(0)\ny = totals['Carjacking'] / totals['sum_bails']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True,random_state=4202024)\n\ndef test_alpha(alphaa):\n    rr = Ridge(alpha=alphaa)\n    pipe = make_pipeline(StandardScaler(), rr)\n    pipe.fit(X_train, y_train)\n    return pipe.score(X_test, y_test)\n\nalphas = np.linspace(0, 1000, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"r2 score\")\n\nbest_alpha = alphas[np.argmax(scores)]\nprint(f\"best alpha: {best_alpha:.4f}\")\nbest_score = np.max(scores)\nprint(f\"best score: {best_score:.4f}\")\n\nbest alpha: 404.0404\nbest score: 0.0101\n\n\n\n\n\n\n\n\n\nThat didn’t really do anything, but maybe there was a chance modifying something like that would have a wonderful effect on performance.\n\n\nDiscussion\nThe relative performance varied dramatically. On one hand, the bag classifier ended up performing really quite well with a recall around 85%. On the other hand, the regression seemed to be a hurculean task for both linear and nonlinear models. One thing that was funny (funny in a sense that I’ll cry about it after I finish writing this sentence) is the CoD score for the ‘best’ decision tree regressor was worse than if it decided to predict the mean every time.\nA handful of the learners considered in this project found TAVG to be a fairly important feature, while I’m pretty sure no learner put a weight or importance on TSUN or PSUN. If any of these learners were higher performing I would dare to attribute this to the court rooms being indoors and therefore not in direct sunlight.\nThere’s a lot of room for improvement in the results, so I have a lot of ideas for ways to get better features. Maybe instead of the weather outside there could be information about the temperature and humidity inside court rooms where the bails were posted. It could also help to have more accurate bail amounts instead of buckets. Another idea would be some sort of severity measure added to the bail like a victim count for a homicide bail. This could be not only more interesting, and morbid, but might help a learner out. And of course this wouldn’t be a valid discussion without the boiler-plate ‘more rows,’ which the city of philadelphia is already doing as a type this sentence."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kyle Wodehouse",
    "section": "",
    "text": "I’m a third year chemical engineering student at the University of Delaware. I’m interested in polymer systems, data science, and signal processing. I’m a research and development co-op at DuPont Water Solutions where I work on ion exchange resin synthesis and applications. I also do a little work with the Jayaraman Lab at UD calculating thermal properties of block copolymer structures with LAMMPS."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Writing",
    "section": "",
    "text": "Philadelphia Needs to Take Heat Seriously\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nKyle Wodehouse\n\n\n\n\n\n\n\n\n\n\n\n\nPress and Paratext\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nKyle Wodehouse\n\n\n\n\n\n\n\n\n\n\n\n\nHypertension for Black Americans in Linda Villarosa’s Everything I Thought was Wrong\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nKyle Wodehouse\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writing/banned books/index.html",
    "href": "writing/banned books/index.html",
    "title": "Press and Paratext",
    "section": "",
    "text": "This was a project for my honors colloquium class “Banned Books”. Putting this together was fun and I visited UD’s special collections to see some actual early print books.\nWhat turns a text into a book? A text is a series of verbal statements glued together with transitions and punctuation. Paratext is everything around the text on the page that affects the reading experience, and gives a text a chance to become a book. Some easy examples of paratext include title pages, fonts, and covers. Paratext elevates a text into a book-lifing a string of words into a literary experience. Early medieval books lacked most presentation and met readers’ eyes in a near-paratext-less form.1 Compare this to a modern book where every single part of the book and everything around the book is part of the experience: bookstores like Barnes and Noble boast coffee shops, book covers are meticulously designed to compliment the text, authors use software like scrivener and latex to format their book to tailor the experience, and the development of modern print technology lets authors put pictures of whatever wherever they want in their text to add to the experience. Paratext developed slowly over a centuries long time span, but the development of press technology catapulted paratext practices towards their modern state.\nParatext divides itself nicely into two broad categories: epitext and peritext. Epitext refers to everything outside the pages that affects how the reader interacts with the book or understands the book like author interviews or even just hearing about the book from other people. Literary awards (Nobel, Pulitzer, and Nebula, to name a few) would also serve as epitext along with the general historical context of the work. The other half of paratext, peritext, is what this work will focus on and refers to the paratext closest to the text and on the pages. Think of peritext like a Christmas gift’s wrapping paper. What would a children’s book be without illustrations?2 Consider a book with no headings and no empty lines, just words on every page. Peritext is how the author and publisher interact with the readers’ experience.\nPeritext is almost perfectly causal: the author, printer, or scribe changes the manuscript or book in a way and their change influences the experience. It might seem unrealistic to paint a deterministic picture like this, but consider titles. A work like by Adam Silvera would have a completely different reading experience if the title portrayed any other message. The power of headings trickles down the hierarchy into chapter titles, headings, and subheadings, too. John Green’s first novel, , labels each chapter with a distance between the chapter and some event, priming the reader for some sort of uber-climatic climax completely separating the ‘before’ from the ‘after.’ Crafting a specific experience is not unique to 21st century writers: renaissance book peddlers often slang small ephemeral prints.3 The small ephemeral books paired with the epitext of the book slinger performing in the streets crafted a unique reading experience, especially when compared to the books kept in academic libraries and read by scholars. Furthermore, paratext does not only live in European texts, and even the first multi-color printed image, a decent landmark in the history of paratext, came from 14th century China.4 All of this goes to say that paratext is inescapable and monumentally impacted by the press.\nThough Elizabeth Eisenstein probably still rolls around in her grave whenever anyone dares to question the impact of the press, the invention and development of the press is a meaningful landmark in the history of paratext. Before Gutenberg worked his magic, manuscripts contained hand-made visual elements like color illuminations. Printed books, particularly those of early print, commonly used woodcut engravings or metal engravings to add visual elements. Engravers carved wood blocks or metal plates and then used them to print illustrations or decorations onto the pages of the book. Early European engravings lacked the intricate detail, personal touch, and color5 of illuminated manuscripts, but they still added interest and embellishment to the printed page.\nFurthermore, the press’s invention marks more than just the end of illuminations; it marks the (slow) start of the commodification of books. With the development of technology like the steam powered printing press books became more efficient and cheap to produce leading to the growth of the book market. Groups gathered in newly founded print shops and brought new marketing and manufacturing techniques to book production.6. In these stores they most likely met a hefty crew of engravers, composers, and press operators collectively churning out printed materials. While this historical image would fall under epitext,7 early print lent itself to small, consistent, wide-spread errors. Issues with books like the’wicked’ bible, a bible that directed readers to commit adultery instead of not commit adultery (Figure 1),8 were present in every copy for each round of printing. For small errors in prints like these, products of one worker’s small mistake, authors circulated errata sheets, pieces of paper detailing the mistakes in the text and the corrections.9 Sadly, errata sheets were not enough to keep readers of the ‘wicked’ bible from committing adultery, and their copies had to be recalled.10\nAn event like the ‘wicked’ bible’s publication could not have happened through the process of scribes hand-writing a copy of another manuscript. If a scribe’s hand slipped and he wrote the wrong word, or if his eyesight was poor and he read the copy wrong, they would only create one manuscript instructing the reader to commit adultery rather than an entire batch. Errata sheets for a hand-made manuscript would need to be uniquely made for each manuscript, which is why they only started shuffling around for printed materials.\nWhen looking at the impact of the press, examining the paratext the author had more direct control over, peritext, will be more rewarding and interesting. The first key factor the author had control of that this work will consider is the typeface. With the standardization of lettering of the press came different standards, most notably gothic and roman typefaces for early print.11 An author of a work of science, like Edward Topsell, might use a Roman typeface to give the work a more serious, academic feel (Figure 2). On the other hand, a work of satire like Sebastian Brant’s allegory of a dysfunctional ship that really talks about dysfunctional governments, , might use a gothic typeface to give the work a more whimsical, light-hearted feel (Figure 2).\nIt seems logical that the removal of handwritten words would also remove an element of paratext, and while that is true to some exteit it also opened up the door for the author and printer to work together to better. In a 1570 printing of by Sebastian Brant, the text is clearly divided into sections of gothic and roman typeface (Figure 4). The standardization of typeface gave authors and printers another degree of creative freedom to present the text in an even more encompassing manner. In a similar manner, because the engravings could be easily reused books could have the consistent initials, the big, decorative letters are the start of sections, throughout the work. Looking at the same 1570 printing of ‘The Ship of Fooles,’ the T initial is consistently reused throughout the work (Figure 5).\nWith standardization and consistency in mind, the press also affected the paratext of early scientific works, like ‘The Historie of Foure-footed Beastes’ by Edward Topsell, by allowing for a standard of spacing between lines and paragraphs. The standardization of spacing allowed for the author to separate sections of the text in a way that would be consistent throughout the work, i.e. consistent margins, and allowed for formal-looking tables to be included. In the same 1570 printing of ‘The Ship of Fooles,’ the author includes a printed table listing the names of four footed beastes (Figure 6).\nEven though reading 16th century English is a near herculean task, it is relatively clear the author split up the contents in these paragraphs differently for some sort of story telling reason (Figure 4). The storytelling strategy of separating sections of a text with typeface carried on since Brant’s time. In her near canonical novel , Octavia Butler puts verse in bold at the start of each chapter. The visual contrast between the boldface and the regular typeface lets her separate the verse from the chapter and, more importantly, make it stand out from the body of the chapter. In Butler’s case, the development of the verse throughout her novel corresponds to a development of philosophy and religion gained from the experiences of the chapter beforehand.\nIn a similar manner, because engravings could be reused, books could have consistent initials, the big, decorative letters at the start of sections throughout the work. Though, the printed works rarely had the same level of size choice as manuscripts at the time where scribes drew larger, more stunning letters to pair with more striking and important divisions in the text.12 For something like a work of science,13 the standardization gives the reader a sense of sophistication and order. On the other hand, the lack of standardization molds a feeling of secrecy and intimacy similar to reading someone’s handwritten diary.\nThough the font was standardized, early print was far from standardized in general. Looking at any of the pictures of the early printed works earlier in this work it is clear there is no standardization of spelling. The random almost humorous spelling found throughout the works pictured earlier seems like it might not even be consistent with itself. However, this small feature in the works does not destroy Elizabeth Eisenstein’s arguments since spelling was also far from standardized in manuscripts.14 The varying spellings were more a feature of the late-manuscript-early-print time period15 rather than a fault of early print itself.\nThough this work has mainly focused on how paratext compliments the reading experience, paratext does more than help the reader interpret and interact with the story. How a book is decorated, both on the outside by bookbinders and the inside by engravers and printers, affects which types of people will buy and read them.16 More sophisticated, intricate works lent themselves to more prestigious owners, and less sophisticated works, like street peddler prints, lent themselves to less prestigious readers.\nEven though paratext changed greatly from the advent of print, this practice still exists in modern print. There is an entire market for decorative books in the 21st century: books whose only purpose is to be a symbol of class and prestige. Additionally, plenty of classic novels exist in cloth bound editions with deckled edges, much fancier than their mass market paperback counterparts.\nWith the print-induced commercialization of books came the necessity for printer reputation. Though the advertisements inside modern books did not appear in early printed works, early prints introduced another name to the title page: the printer. Early manuscripts were largely fueled by commission,17 but with the switch to print and the birth of the print market18 printers were not able to wait around for customers to commission books for them since they needed to stock their shelves. Putting their name on the title page, larger than the author of the text in the case of , was a method of building a customer base by making sure the reader knew who printed the book.\nExploring paratext reveals the layers that transform a text into a book, shaping the reading experience and engaging the reader in ways beyond the words on the page. From the historical roots of paratext to its evolution alongside printing technology, and printing technology’s landmark effect on paratext, this work delved into how elements like title pages, fonts, illustrations, and even errata sheets contribute to the literary encounter. The distinction between epitext and peritext elucidates the broader context surrounding a book and the elements directly influencing the reading process. Through epitext, encompassing author interviews, literary awards, and historical context, readers encounter a book’s narrative before even turning its pages. Peritext, on the other hand, is the immediate environment of the text, akin to the wrapping paper of a gift, influencing the reader’s journey within the book itself.\nPrinting technology not only revolutionized the dissemination of knowledge19 but also introduced new dimensions to paratext. Standardized typefaces, consistent spacing, and reusable engravings provided authors and printers with creative tools to enhance the presentation of their work. Whether through the choice of typeface or the layout of illustrations, paratext became a means for authors and printers to craft specific reading experiences tailored to their narrative goals rather than leaving creative choices to a scribe who the author likely has no relation to. It also extends beyond aesthetics, playing a crucial role in shaping the book’s reception and audience. The visual appeal of a book, both in its exterior design by bookbinders and interior embellishments by engravers, influences its perceived value and attracts diverse readerships.\nAnalyzing paratext offers an avenue for understanding the relationship between text, reader, and the broader culture. By examining case studies and examples the nuance of paratextual elements and their impact on the reading experience is unraveled, shedding light on the interplay between form and content in the world of books."
  },
  {
    "objectID": "writing/banned books/index.html#footnotes",
    "href": "writing/banned books/index.html#footnotes",
    "title": "Press and Paratext",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGérard Genette, Paratexts: Thresholds of Interpretation, Literature, Culture, Theory 20 (Cambridge ; New York, NY, USA: Cambridge University Press, 1997), 3.↩︎\nAnswer: a very, very sad childrens book.↩︎\nRosa M. Salzberg, “\"Selling Stories and Many Other Things in and Through the City\": Peddling Print in Renaissance Florence and Venice,” The Sixteenth Century Journal 42, no. 3 (2011): 737–59, http://www.jstor.org/stable/23076489.↩︎\nMichael Sullivan, The Arts of China, 3rd ed (Berkeley: University of California Press, 1984), 203.↩︎\nThis, of course, only applies for early European print until color prints showed up in the form of chiaroscuro woodcuts after their 1508 invention by Lucas Cranach the Elder in Germany.↩︎\nElizabeth L. Eisenstein, The Printing Revolution in Early Modern Europe, 2. ed., Canto classics ed, Canto Classics (Cambridge: Cambridge Univ. Press, 2012), 22–23.↩︎\nfigure out something interesting to say here↩︎\nEisenstein, The Printing Revolution in Early Modern Europe, 56.↩︎\nEisenstein, 56–57.↩︎\nLuckily around a dozen copies are still kicking it today in the special collections of universities like Oxford, Cambridge, Princeton, and Yale, as well as the New York Public Library’s rare books collection.↩︎\nEisenstein, The Printing Revolution in Early Modern Europe, 59.↩︎\nRuth Carroll et al., “Pragmatics on the Page,” European Journal of English Studies 17, no. 1 (April 1, 2013): 59, https://doi.org/10.1080/13825577.2013.755006.↩︎\nAdmittedly, the history of four footed beasts is not a good example of a ‘scientific’ work since a decent amount of the four footed beasts are fictional, including the one on the title page.↩︎\nEisenstein, The Printing Revolution in Early Modern Europe, 348.↩︎\n‘the time period’ is a very vague phrase to use, but it means the time before things like webster’s dictionary (mid 18th century) or early style guides (chicago manual first published in early 20th century) guided authors and printers on how to spell or how to cite.↩︎\nCarroll et al., “Pragmatics on the Page,” 64.↩︎\nSirkku Ruokkeinen and Aino Liira, “Material Approaches to Exploring the Borders of Paratext,” Textual Cultures 11, no. 1 (2017): 125, https://www.jstor.org/stable/26662794.↩︎\nsee earlier remarks about the birth of the book market↩︎\nSorry to all the Adrian Johns supporters who live to invalidate Eisenstein’s career.↩︎"
  }
]