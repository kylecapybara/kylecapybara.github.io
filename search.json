[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Philadelphia Needs to Take Heat Seriously\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nKyle Wodehouse\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/weatherbails/index.html",
    "href": "projects/weatherbails/index.html",
    "title": "How does Philadelphia weather affect bails?",
    "section": "",
    "text": "Introduction\nData sourced from the city of Philadelphia. This is a data set that includes information about what bails were set and for what crime over the entire city of Philadelphia from 2014 to 2022. Data about daily weather in Philadelphia (specifically measured at the Philadelphia Airport) sourced from National Centers for Environmental Information. The formal documentation lists out what each column in the weather dataset means, so I’ll leave the column names as they are to not stray from the documentation.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nbails = pd.read_csv('bail_data_daily_citywide.txt')\nweather = pd.read_csv('daily_weather_data.csv')\n\nI’ll merge the sets in the same manner as project 1, and fill NaN values with 0 as none of the values important to this analysis will be affected by filling NaN with 0.\n\ntotals = bails[bails['bail_category'] == 'Total']\nbails = bails[bails['bail_category'] != 'Total']\n\n#saving the list of columns for later !!\nbail_columns = bails.columns[2:]\n\nbails = bails.merge(weather, left_on='date_value', right_on='DATE', how='left')\n\n#getting rid of some useless columns\nbails.drop(columns=['DATE', 'STATION', 'NAME'], inplace=True)\nbails = bails.fillna(0)\n\nprint(bails.columns)\n\nIndex(['date_value', 'bail_category', 'Aggravated Assault',\n       'Altered Firearm Serial Number', 'Arson', 'Attempted Murder',\n       'Auto Theft', 'Burglary/Commercial', 'Burglary/Residential',\n       'Carjacking', 'Criminal Mischief', 'Disorderly Conduct',\n       'Drug Possession', 'Drug Possession in Jails', 'Drug Sales',\n       'Drug Sales with a Firearm', 'DUI', 'Ethnic Intimidation',\n       'Firearm Possession by a Prohibited Person',\n       'Firearm Possession without a License', 'Fraud', 'Homicide: Other',\n       'Homicide: Shooting', 'Illegal Dumping/Littering', 'Non-Fatal Shooting',\n       'Other Assaults', 'Other Firearm Offenses', 'Other Property Crimes',\n       'Other Violent Crimes', 'Patronizing Prostitutes/Sex Workers',\n       'Promoting Prostitution', 'Prostitution/Sex Work', 'Rape',\n       'Retail Theft', 'Robbery', 'Robbery with a Deadly Weapon',\n       'Sexual Assault and Other Sex Offenses', 'Simple Assault',\n       'Strangulation', 'Theft', 'Theft from Auto', 'Threats of Violence',\n       'Trespass', 'Uncategorized Offenses',\n       'Victim/Witness Intimidation & Retaliation',\n       'Violation of Protection Order', 'AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW',\n       'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5',\n       'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08', 'WT09'],\n      dtype='object')\n\n\nI’ll also quick just describe the weather columns to get an idea of what they’re looking like\n\nbails[['AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW',\n       'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5',\n       'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08', 'WT09']].describe()\n\n\n\n\n\n\n\n\nAWND\nPGTM\nPRCP\nPSUN\nSNOW\nSNWD\nTAVG\nTMAX\nTMIN\nTSUN\n...\nWSF2\nWSF5\nWT01\nWT02\nWT03\nWT04\nWT05\nWT06\nWT08\nWT09\n\n\n\n\ncount\n24256.000000\n24256.000000\n24256.000000\n24256.0\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.0\n...\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n24256.000000\n\n\nmean\n9.039241\n0.201847\n0.127503\n0.0\n0.069360\n0.151880\n54.852243\n65.370712\n48.519789\n0.0\n...\n19.786972\n26.166260\n0.344987\n0.026715\n0.080805\n0.013852\n0.000989\n0.009894\n0.094327\n0.002309\n\n\nstd\n3.706535\n11.112817\n0.359605\n0.0\n0.636483\n0.968468\n21.252877\n18.588487\n17.187521\n0.0\n...\n6.645629\n8.328464\n0.475374\n0.161253\n0.272541\n0.116880\n0.031441\n0.098980\n0.292289\n0.047994\n\n\nmin\n0.670000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n0.000000\n13.000000\n2.000000\n0.0\n...\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n6.490000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n42.000000\n50.000000\n34.000000\n0.0\n...\n15.000000\n19.900000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n8.500000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n58.000000\n67.000000\n48.000000\n0.0\n...\n18.100000\n25.100000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n10.960000\n0.000000\n0.060000\n0.0\n0.000000\n0.000000\n73.000000\n82.000000\n64.000000\n0.0\n...\n23.000000\n30.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26.620000\n612.000000\n4.760000\n0.0\n19.400000\n18.100000\n90.000000\n98.000000\n82.000000\n0.0\n...\n55.000000\n72.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 22 columns\n\n\n\nNow, I’ll make some pairplots to visualize the relationships between ethnic intimidation and all the weather data. This will come in handy when I do some data hengineering\n(this part removed for github upload)\n\n\nClassification\nI will try and answer this question:\nCan data about the average wind speed, peak gust time, precipitations, daily percent of possible sunshine, snowfall, snow depth, average temperature, max temperature, min temperature, total minutes of sunshine, direction of fastest wind for 2 and 5 minute periods, fastest wind speed in 2 and 5 minutes, and weather types (classes) predict whether 2 or more ethnic intimidation bails were posted for $25-$100k?\nI’ll choose recall as my metric because recall is effectively ‘out of all the true cases what proportion did it identify,’ and in the context of this question it makes a lot of sense to gague the learner’s ability based on correctly identifying the true cases and ‘punish’ the learner for false negatives.\nNow, I’ll setup my \\(\\textbf{X}\\) and \\(\\textbf{y}\\) and split them into training and testing sets with random state of 4202024 because today is 4/20/2024.\n\nX = bails.loc[(bails['bail_category'] == '$25k-$100k') & (bails['Drug Sales'] &gt; 0)][['AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW',\n       'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5',\n       'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08', 'WT09']]\ny = bails.loc[(bails['bail_category'] == '$25k-$100k') & (bails['Drug Sales'] &gt; 0)]['Drug Sales'] &gt; 1\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True,random_state=4202024)\n\nNow I’ll go through a grid search over a handful of values for max depth and minimum samples per leaf on some decision tree classifiers.\n\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\n\ngrid = { \"max_depth\":[6,8,10,12,14], \n         \"min_samples_leaf\":[1,2,3,4,7]}\nlearner = DecisionTreeClassifier(random_state=420)\nkf = StratifiedKFold(n_splits=8, shuffle=True, random_state=2024)\n\ngrid_search = GridSearchCV(\n    learner, grid, \n    scoring=\"recall\", \n    cv=kf,\n    n_jobs=-1\n    )\ngrid_search.fit(X_train, y_train)\n\ntree_best_params = grid_search.best_params_\ntree_best_score = grid_search.best_score_\nprint(tree_best_params)\nprint(tree_best_score)\n\n{'max_depth': 8, 'min_samples_leaf': 3}\n0.6368873742291463\n\n\nThat’s depressing. Now I’ll do a similar grid search for a kNN learner with\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ngrid = {\n    'n_neighbors': range(1, 21, 2),\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan', 'minkowski'],\n    'leaf_size': list(range(1,50)),\n}\nlearner = KNeighborsClassifier()\nkf = StratifiedKFold(n_splits=8, shuffle=True, random_state=302)\n\ngrid_search = GridSearchCV(\n    learner, grid, \n    scoring=\"recall\", \n    cv=kf,\n    n_jobs=-1\n    );\ngrid_search.fit(X_train, y_train);\n\nkNN_best_params = grid_search.best_params_\nkNN_best_score = grid_search.best_score_\nprint(kNN_best_params)\nprint(kNN_best_score)\n\n{'leaf_size': 1, 'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n0.5464135021097046\n\n\nNext I’ll put together a decision tree with the best parameters from the grid search.\n\nbest_tree = DecisionTreeClassifier(**tree_best_params)\nbest_tree.fit(X_train, y_train)\n\nfeature_importances = best_tree.feature_importances_\ncolumns = X_train.columns\nfeature_importances = pd.Series(feature_importances, index=columns)\nfeature_importances = feature_importances.sort_values(ascending=True)\nprint(feature_importances)\n\nWT09    0.000000\nWT03    0.000000\nWT01    0.000000\nWT04    0.000000\nWT06    0.000000\nTSUN    0.000000\nSNOW    0.000000\nPSUN    0.000000\nPGTM    0.000000\nWT05    0.000000\nWT02    0.011778\nSNWD    0.015576\nWT08    0.020982\nPRCP    0.031532\nWSF2    0.057685\nWDF5    0.059225\nTMAX    0.083542\nWSF5    0.093361\nTAVG    0.115266\nWDF2    0.142408\nTMIN    0.168358\nAWND    0.200287\ndtype: float64\n\n\nit’s a little sad the learner doesn’t care much about the sun or snow, but it looks like there might be something going on with the temperature.\nNow, I’ll see if I can put together a whole bunch of these admittedly trash classifiers to make one mega classifier.\n\nfrom sklearn.discriminant_analysis import StandardScaler\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.pipeline import make_pipeline\n\nknn = KNeighborsClassifier(**kNN_best_params)\npipe = make_pipeline(StandardScaler(), knn)\nbag = BaggingClassifier(pipe, n_estimators=50, max_samples=0.15, oob_score=True, random_state=302)\nbag.fit(X_train, y_train)\n\nfrom sklearn.metrics import classification_report\ny_pred = bag.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n       False       0.42      0.46      0.44       127\n        True       0.60      0.55      0.58       182\n\n    accuracy                           0.52       309\n   macro avg       0.51      0.51      0.51       309\nweighted avg       0.53      0.52      0.52       309\n\n\n\nThats looking marginally better! Lets look at some validation curves. First, lets look at changing the max_features\n\nfrom sklearn.model_selection import ValidationCurveDisplay\n\nValidationCurveDisplay.from_estimator(\n    bag, X, y,\n    param_name=\"max_features\", \n    param_range=np.linspace(0, 0.5, 20),\n    cv=StratifiedKFold(n_splits=6, shuffle=True, random_state=19716),\n    scoring=\"recall\",\n    n_jobs=-1\n    )\n\n\n\n\n\n\n\n\nI only did the max_samples from 0-0.2 because realistically showing each learner 20% of the data would already make them too correlated with eachother.\nNow I’ll set the max features to 0.3 because that’s where it seems to steady out. Let’s look at max_samples\n\nbag = BaggingClassifier(pipe, n_estimators=50, max_samples=0.15,max_features=0.3, oob_score=True, random_state=302)\nbag.fit(X_train, y_train)\n\nValidationCurveDisplay.from_estimator(\n    bag, X, y,\n    param_name=\"max_samples\", \n    param_range=np.linspace(0, 0.5, 10),\n    cv=StratifiedKFold(n_splits=6, shuffle=True, random_state=19716),\n    scoring=\"recall\",\n    n_jobs=-1\n    )\n\n\n\n\n\n\n\n\nNow lets look at how the 0.1 max_samples learner performs more closely to see if it performs better than the best decision tree\n\nbag = BaggingClassifier(pipe, n_estimators=50, max_samples=0.1,max_features=0.3, oob_score=True, random_state=302)\nbag.fit(X_train, y_train)\n\nprint(classification_report(y_test, best_tree.predict(X_test)))\nprint(classification_report(y_train, bag.predict(X_train)))\n\n              precision    recall  f1-score   support\n\n       False       0.41      0.33      0.37       127\n        True       0.59      0.66      0.62       182\n\n    accuracy                           0.53       309\n   macro avg       0.50      0.50      0.49       309\nweighted avg       0.51      0.53      0.52       309\n\n              precision    recall  f1-score   support\n\n       False       0.57      0.41      0.48       606\n        True       0.55      0.71      0.62       628\n\n    accuracy                           0.56      1234\n   macro avg       0.56      0.56      0.55      1234\nweighted avg       0.56      0.56      0.55      1234\n\n\n\nIt looks like the bag is performing quite well compared to the measly best tree. Let’s look at the ROC curve for the bag\n\nfrom sklearn.metrics import roc_curve\n\nresults = []\nactual = y_test\np_hat = bag.predict_proba(X_test)\nfp, tp, theta = roc_curve(actual,p_hat[:,1])\nresults.extend( [('satisfied',fp,tp) for fp,tp in zip(fp,tp)] )\nroc = pd.DataFrame( results, columns=[\"label\",\"FP rate\",\"TP rate\"] )\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None\n    )\n\n\n\n\n\n\n\n\nWell, that’s not looking great, but it’s not looking disastrous.\n\n\nRegression\nquestion: can I create a model to predict what proportion of all bails posted were from car theft using the same weather data as the classification section (average wind speed, peak gust time, precipitations, daily percent of possible sunshine, snowfall, snow depth, average temperature, max temperature, min temperature, total minutes of sunshine, direction of fastest wind for 2 and 5 minute periods, fastest wind speed in 2 and 5 minutes, and weather types (classes)).\nI’ll start by putting together a list of the features to feed to some models with the targets\n\nprint(totals)\n\ntotals['sum_bails'] = totals[bail_columns].sum(axis=1)\n\ntotals = totals.merge(weather, left_on='date_value', right_on='DATE', how='left')\n\n       date_value bail_category  Aggravated Assault   \n0      2014-01-01         Total                  18  \\\n9      2014-01-02         Total                  38   \n18     2014-01-03         Total                  13   \n27     2014-01-04         Total                  12   \n36     2014-01-05         Total                   5   \n...           ...           ...                 ...   \n27243  2022-04-16         Total                  10   \n27252  2022-04-17         Total                   6   \n27261  2022-04-18         Total                   9   \n27270  2022-04-19         Total                  10   \n27279  2022-04-20         Total                   4   \n\n       Altered Firearm Serial Number  Arson  Attempted Murder  Auto Theft   \n0                                  0      0                 2           4  \\\n9                                  1      0                 4           3   \n18                                 0      0                 0           2   \n27                                 0      0                 0           5   \n36                                 0      0                 1           0   \n...                              ...    ...               ...         ...   \n27243                              0      2                 0           4   \n27252                              1      0                 0          11   \n27261                              0      0                 0           1   \n27270                              0      1                 0           2   \n27279                              0      0                 0           1   \n\n       Burglary/Commercial  Burglary/Residential  Carjacking  ...   \n0                        1                     3           0  ...  \\\n9                        0                     3           1  ...   \n18                       1                     1           0  ...   \n27                       0                     2           0  ...   \n36                       0                     0           0  ...   \n...                    ...                   ...         ...  ...   \n27243                    1                     2           0  ...   \n27252                    0                     2           1  ...   \n27261                    0                     2           1  ...   \n27270                    6                     2           0  ...   \n27279                    0                     1           0  ...   \n\n       Sexual Assault and Other Sex Offenses  Simple Assault  Strangulation   \n0                                          1               8              0  \\\n9                                          1               6              0   \n18                                         1               5              0   \n27                                         0               1              0   \n36                                         1               6              0   \n...                                      ...             ...            ...   \n27243                                      0               2              1   \n27252                                      0               2              1   \n27261                                      0               4              1   \n27270                                      0               2              0   \n27279                                      0               1              1   \n\n       Theft  Theft from Auto  Threats of Violence  Trespass   \n0          4                0                    1         0  \\\n9          6                0                    2         0   \n18         0                0                    2         2   \n27         6                0                    1         0   \n36         3                0                    0         0   \n...      ...              ...                  ...       ...   \n27243      3                0                    1         0   \n27252      1                0                    0         1   \n27261      0                0                    0         2   \n27270      2                0                    0         3   \n27279      2                0                    0         0   \n\n       Uncategorized Offenses  Victim/Witness Intimidation & Retaliation   \n0                           4                                          0  \\\n9                           6                                          1   \n18                          1                                          0   \n27                          5                                          0   \n36                          2                                          0   \n...                       ...                                        ...   \n27243                       7                                          0   \n27252                       0                                          0   \n27261                       2                                          0   \n27270                       1                                          0   \n27279                       2                                          0   \n\n       Violation of Protection Order  \n0                                  2  \n9                                  4  \n18                                 1  \n27                                 0  \n36                                 0  \n...                              ...  \n27243                              1  \n27252                              0  \n27261                              0  \n27270                              1  \n27279                              0  \n\n[3032 rows x 46 columns]\n\n\nNow that all that is set up, it’s time to get this data stew brewing. I’ll use \\(R^2\\) as my metric because I’m boring. Let’s jump into some ridge regression:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\n\nX = totals[['AWND', 'PGTM', 'PRCP',\n       'PSUN', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5',\n       'WSF2', 'WSF5', 'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08',\n       'WT09']].fillna(0)\ny = totals['Carjacking'] / totals['sum_bails']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True,random_state=4202024)\n\n\nrr = Ridge(alpha=0.5)\npipe = make_pipeline(StandardScaler(), rr)\npipe.fit(X_train, y_train)\nprint(f\"ridge CoD score: {pipe.score(X_test, y_test):.4f}\")\n\nridge CoD score: 0.0064\n\n\nThat score is like… terrible. To see is this is just a bad initial alpha selection I’ll graoh the CoD as a function of \\(\\alpha\\)\n\ndef test_alpha(alphaa):\n    rr = Ridge(alpha=alphaa)\n    pipe = make_pipeline(StandardScaler(), rr)\n    pipe.fit(X_train, y_train)\n    return pipe.score(X_test, y_test)\n\nalphas = np.linspace(0, 1000, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"r2 score\")\n\nbest_alpha = alphas[np.argmax(scores)]\nprint(f\"best alpha: {best_alpha:.4f}\")\nbest_score = np.max(scores)\nprint(f\"best score: {best_score:.4f}\")\n\nbest alpha: 404.0404\nbest score: 0.0101\n\n\n\n\n\n\n\n\n\nThat smooth curve is really beautiful, but all of these \\(R^2\\) scores are trash. Even though this is an absolutely terrible model, let’s look under the hood and see these feature weights\n\nrr = Ridge(alpha=best_alpha)\npipe = make_pipeline(StandardScaler(), rr)\npipe.fit(X_train, y_train)\n\nweights_ridge = pd.Series(pipe.named_steps['ridge'].coef_, index=X_test.columns)\nweights_ridge = weights_ridge.sort_values(ascending=True)\nprint(weights_ridge)\n\nWT04   -0.000299\nTMIN   -0.000293\nWDF5   -0.000225\nTMAX   -0.000204\nSNOW   -0.000188\nWT09   -0.000138\nWSF2   -0.000136\nWT03   -0.000123\nPGTM   -0.000096\nWT05   -0.000053\nSNWD   -0.000048\nWT01   -0.000014\nPSUN    0.000000\nTSUN    0.000000\nWSF5    0.000008\nWT02    0.000025\nWT08    0.000028\nPRCP    0.000136\nAWND    0.000311\nWT06    0.000337\nWDF2    0.000546\nTAVG    0.000565\ndtype: float64\n\n\nLooks like average temperature has the highest influence which might mean (admittedly it is WILD to harvest any meaning from this model) that during warmer days a higher proportion of bails posted were car jackings. This might explain why my AP stats teacher got his catalytic converter stolen early last summer.\nLet’s take a peek at some LASSO now\n\nfrom sklearn.linear_model import Lasso\nlass = Lasso(alpha=0.5)\nlass.fit(X_train, y_train)\nweights_lass = pd.Series(lass.coef_, index=X_train.columns)\nprint(weights_lass)\ndropped = weights_lass[weights_lass == 0].index\nprint('dropped cols:' + '\\n', dropped)\n\nAWND    0.0\nPGTM   -0.0\nPRCP    0.0\nPSUN    0.0\nSNOW   -0.0\nSNWD   -0.0\nTAVG    0.0\nTMAX   -0.0\nTMIN   -0.0\nTSUN    0.0\nWDF2    0.0\nWDF5    0.0\nWSF2    0.0\nWSF5    0.0\nWT01   -0.0\nWT02    0.0\nWT03   -0.0\nWT04   -0.0\nWT05   -0.0\nWT06    0.0\nWT08   -0.0\nWT09   -0.0\ndtype: float64\ndropped cols:\n Index(['AWND', 'PGTM', 'PRCP', 'PSUN', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN',\n       'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5', 'WT01', 'WT02', 'WT03', 'WT04',\n       'WT05', 'WT06', 'WT08', 'WT09'],\n      dtype='object')\n\n\nI don’t think thats supposed to look like that….. I’ll plot the sum of the absolute values of the weights as a function of alpha to see if theres any alpha value that gets some non-zero weights.\n\ndef test_alpha(alphaa):\n    lass = Lasso(alpha=alphaa)\n    lass.fit(X_train, y_train)\n    return abs(lass.coef_).sum()\n\nalphas = np.linspace(0, 10, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"sum abs(coeff_)\")\n\nText(0, 0.5, 'sum abs(coeff_)')\n\n\n\n\n\n\n\n\n\nthat’s also not looking too promising, but I’ll zoom in near zero to see what’s going on.\n\nalphas = np.linspace(0, 0.001, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"sum abs(coeff_)\")\n\nText(0, 0.5, 'sum abs(coeff_)')\n\n\n\n\n\n\n\n\n\nIt looks like the LASSO’s ability to smoosh weights down to 0 squishes everything down to 0 for even slightly non-zero \\(\\alpha\\) values\nenough with all these \\(lines\\) I’ve been hearing about. let’s take a trip back to the forest (look at some trees (do some non-linear regression with a decision tree regressor))\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ngrid = { \"max_depth\":[2,6,8,10,12,14], \n         \"min_samples_leaf\":[2,3,4,7]}\nlearner = DecisionTreeRegressor(random_state=420)\n\ngrid_search = GridSearchCV(\n    learner, grid, \n    scoring=\"r2\", \n    n_jobs=-1\n    )\ngrid_search.fit(X_train, y_train)\n\ntree_best_params = grid_search.best_params_\ntree_best_score = grid_search.best_score_\nprint(tree_best_params)\n\ntreez = DecisionTreeRegressor(**tree_best_params)\ntreez.fit(X_train, y_train)\nCofD_dtr = treez.score(X_test, y_test)\nprint(f\"Decision Tree Regressor r2 score: {CofD_dtr:.4f}\")\ntop_feature = X_train.columns[np.argmax(treez.feature_importances_)]\nprint(f\"Most important feature: {top_feature}\")\n\n{'max_depth': 2, 'min_samples_leaf': 2}\nDecision Tree Regressor r2 score: -0.0018\nMost important feature: TAVG\n\n\nThe forest failed me. As one last thing to try and get a half decent result, I’ll do a little data \\(hengineering\\) and make some new columns that are functions of other ones (non-zero entries of course).\n\naddnl_columns = []\nfor column in totals.columns:\n    if type(totals[column][0]) == float:\n        totals[f\"log_{column}\"] = np.log(totals[column])\n        totals[f\"sqrt_{column}\"] = np.sqrt(totals[column])\n        totals[f\"exp_{column}\"] = np.exp(totals[column])\n        totals[f\"inv_{column}\"] = 1 / totals[column]\n        addnl_columns.append(f\"log_{column}\")\n        addnl_columns.append(f\"sqrt_{column}\")\n        addnl_columns.append(f\"exp_{column}\")\n        addnl_columns.append(f\"inv_{column}\")\ntotals['prod_tavgmax'] = totals['TAVG'] * totals['TMAX']\n\n\nX = totals[['AWND', 'PGTM', 'PRCP',\n       'PSUN', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5',\n       'WSF2', 'WSF5', 'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT08',\n       'WT09'] + addnl_columns].fillna(0)\ny = totals['Carjacking'] / totals['sum_bails']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True,random_state=4202024)\n\ndef test_alpha(alphaa):\n    rr = Ridge(alpha=alphaa)\n    pipe = make_pipeline(StandardScaler(), rr)\n    pipe.fit(X_train, y_train)\n    return pipe.score(X_test, y_test)\n\nalphas = np.linspace(0, 1000, 100)\nscores = [test_alpha(alpha) for alpha in alphas]\nplt.plot(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"r2 score\")\n\nbest_alpha = alphas[np.argmax(scores)]\nprint(f\"best alpha: {best_alpha:.4f}\")\nbest_score = np.max(scores)\nprint(f\"best score: {best_score:.4f}\")\n\nbest alpha: 404.0404\nbest score: 0.0101\n\n\n\n\n\n\n\n\n\nThat didn’t really do anything, but maybe there was a chance modifying something like that would have a wonderful effect on performance.\n\n\nDiscussion\nThe relative performance varied dramatically. On one hand, the bag classifier ended up performing really quite well with a recall around 85%. On the other hand, the regression seemed to be a hurculean task for both linear and nonlinear models. One thing that was funny (funny in a sense that I’ll cry about it after I finish writing this sentence) is the CoD score for the ‘best’ decision tree regressor was worse than if it decided to predict the mean every time.\nA handful of the learners considered in this project found TAVG to be a fairly important feature, while I’m pretty sure no learner put a weight or importance on TSUN or PSUN. If any of these learners were higher performing I would dare to attribute this to the court rooms being indoors and therefore not in direct sunlight.\nThere’s a lot of room for improvement in the results, so I have a lot of ideas for ways to get better features. Maybe instead of the weather outside there could be information about the temperature and humidity inside court rooms where the bails were posted. It could also help to have more accurate bail amounts instead of buckets. Another idea would be some sort of severity measure added to the bail like a victim count for a homicide bail. This could be not only more interesting, and morbid, but might help a learner out. And of course this wouldn’t be a valid discussion without the boiler-plate ‘more rows,’ which the city of philadelphia is already doing as a type this sentence."
  },
  {
    "objectID": "projects/304/index.html",
    "href": "projects/304/index.html",
    "title": "CHEG304 Python",
    "section": "",
    "text": "Every UD chemical engineering student takes CHEG304 “Random Variability in Chemical Processes”. The professor is lovely, but they were taught matlab and therefore share helpful matlab functions. Over a week I put together this pamphlet citing the functions our professor shared and then pointing to python functions that are functionally similar if not the exact same. I also included tons of examples of how to use some of the functions along with plenty of helpful warnings and notes.\nThe book is free to access here"
  },
  {
    "objectID": "projects/proj.html",
    "href": "projects/proj.html",
    "title": "Projects",
    "section": "",
    "text": "CHEG304 Python\n\n\nProviding a resource for doing CHEG304 coursework with python instead of matlab\n\n\n\nstatistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Best Seat at UD\n\n\nAn observational study on the chairs and desks around UD’s campus\n\n\n\nstatistics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcess Flow Diagram Maker\n\n\na small tkinter program to draw process flow diagrams and generate TikzPicture code\n\n\n\nlatex\n\n\ngui\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDid JK Rowling’s Writing Change When She Switched Her Genre?\n\n\nUsing NLTK to create features and common clustering algorithms on Harry Potter text\n\n\n\nclustering\n\n\nmath219\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow does Philadelphia weather affect bails?\n\n\nUsing data from the city of Philadelphia about bails and weather to invesigate their relationship with common classification and regression algorithms.\n\n\n\nclassification\n\n\nregression\n\n\nmath219\n\n\npython\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/heat/index.html",
    "href": "posts/heat/index.html",
    "title": "Philadelphia Needs to Take Heat Seriously",
    "section": "",
    "text": "Last October I saw Alex G in concert in Philadelphia at Union Transfer, a smaller venue in Philly’s Callowhill neighborhood. Callowhill sits at a uniquely liminal position in the Philadelphia neighborhood puzzle–just north of Chinatown and the Vine Street Expressway, and just south of the Richard Allen Homes project, a majority Black, affordable housing community. On a broader scale, Callowhill finds itself in the awkwardness that is Philadelphia between Center City and North Philadelphia. In a different life the neighborhood bustled with blue collar worker filled factories. Factory buildings transformed into music venues like Union Transfer and Franklin Music Hall and hip loft-style housing, and more affluent tenants snuffed out the blue collar population.\nUnion Transfer was a fantastic place to hear my favorite tracks like ‘Hope,’ a song about Alex’s formative young adult experiences in Philadelphia, and ‘Snot,’ my personal favorite. Filled to the brim with concertgoers, the venue got toasty, but I only broke a sweat on the walk to get some pizza afterwards. The jet black roads, cracked cement sidewalks, and weathered brick buildings soaked up heat all day, and the heat lingered long after the sunset. This unusual warmness of the city, formally known as the ‘heat island’ effect, is an issue for all cities, but especially for cities with too few trees.\n\n\n\nA street in Callowhill\n\n\nOn top of filling streets with their big green beauty, trees also manage how much of the sun’s rays the ground can absorb, and cool down their soil through evapotranspiration. All this fights off the heat island effect. Trees are even more effective than other ground coverings like bushes or grass. The continued worsening of the heat island effect increases the need for more trees in urban areas.\nThe issue is poor neighborhoods have less trees than wealthy neighborhoods. Formerly redlined areas–areas that received severely limited real estate investment because the government allowed banks to consider lending to Black people ‘risky’—have fewer environmental amenities to help clean and cool the air, like trees. The city of Philadelphia’s own interactive report on over 100,000 trees visually highlights the disparity. Trees evenly dot Fairmount and other uber wealthy neighborhoods, but run thin just north in Strawberry Mansion where the average household income is over $80,000 lower. And while the distribution is inequitable when considering economic class, trees in Philadelphia also stratify along lines of race—Strawberry Mansion is 98% Black.\n\n\n\nDistribution of asthma cases in North Philly (darker means higher)\n\n\n\n\n\nPercent of population living in poverty (darker means higher)\n\n\nOf course, tree disparity matters when talking about defending a community from the heat island effect and shielding sidewalks from harsh sunlight, but it mixes with poor neighborhoods having less access to air conditioning, higher rates of asthma, and higher rates of diabetes to make an unjust cocktail. So while rich neighborhoods walk tree-protected streets, the sun shines harder on less fortunate neighborhoods.\nAnd the sun will only shine harder–over the past 60 years heat waves’ intensity, duration, and frequency grew. This means hotter temperatures for longer and more often, a damning future for low income homes. Though the present is damning as well considering increasing tree cover to 30% could save 400, mostly low-income, Philadelphians from dying prematurely each year. Heat’s deadliness extends outside of cities killing more people than hurricanes, tornadoes, and lightning, and with overall rising temperatures–July 2023 being the hottest month in human history–the death toll of heat will only rise.\nTo address this issue Philadelphia needs more minds like Erica Fichman, current manager of TreePhilly. TreePhilly offers free street tree installation, grants to host tree planting community events, and tree maintenance for up to a year after planting. These grants for community tree planting events speak to the community building nature of planting trees–creating a sense of place where homeowners walk home and see trees they remember planting.\nLuckily Philadelphia recently received 12 million dollars in USDA funding from a massive government grant focused on increasing urban greenery in an equitable fashion. When spending their once in a lifetime sum of money, Philly needs to acknowledge the complex reasons low-income renters and homeowners aren’t already requesting street and yard trees. These include uncertainty around who will care for the tree, who will spend hours dialing every arborist in the phonebook until one finally agrees to prune the tree for thousands of dollars, and who will be responsible for paying to repair sidewalk damage from the roots in 5 years. And Philly needs to prepare a response for when homeowners rightfully ask if their homeownership will be protected from tax hikes from property value increases. This means hiring arborists to prune and care for planted trees, offering sidewalk repair grants, and building policy to protect freshly planted areas from tax hike driven gentrification.\nBy recognizing and actively addressing systemic barriers to tree access and maintenance, Philadelphia can forge a path toward more equitable urban development and effective use of new USDA funds."
  },
  {
    "objectID": "projects/pfd maker/index.html",
    "href": "projects/pfd maker/index.html",
    "title": "Process Flow Diagram Maker",
    "section": "",
    "text": "Process Flow Diagrams (PFDs) are a useful tool in visualizing chemical engineering processes, but making them with tikzpictures is very annoying and slow. But if you use tikzpictures/latex the diagram will be rendered as a vector graphic so you can zoom in without losing image quality. This project works at making them as easily as you’d edit a google slide show or something.\nHere are some examples of PDFs I made with the help of my gui (the whole document is here)\n   \na writeup about the controls and the python file for this project can be found here"
  },
  {
    "objectID": "projects/harrpotter/index.html",
    "href": "projects/harrpotter/index.html",
    "title": "Did JK Rowling’s Writing Change When She Switched Her Genre?",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport nltk\n\nharry = pd.read_csv('harry_potter_books.csv')\nharry.head()\n\n\n\n\n\n\n\n\ntext\nbook\nchapter\n\n\n\n\n0\nTHE BOY WHO LIVED Mr. and Mrs. Dursley, of nu...\nBook 1: Philosopher's Stone\nchap-1\n\n\n1\nfour, Privet Drive, were proud to say that the...\nBook 1: Philosopher's Stone\nchap-1\n\n\n2\nthank you very much. They were the last people...\nBook 1: Philosopher's Stone\nchap-1\n\n\n3\nbe involved in anything strange or mysterious,...\nBook 1: Philosopher's Stone\nchap-1\n\n\n4\nwith such nonsense. Mr. Dursley was the direc...\nBook 1: Philosopher's Stone\nchap-1\n\n\n\n\n\n\n\nEach row contains some text (probably just the literal lines from whatever ebook this guy scraped the text from) and also notes the book it is from and what chapter the text is from as well. Let’s make a new column that is the number of words in each row using a very, very tasty library called NLTK.\n\nharry['word count'] = harry['text'].apply(lambda x: len(nltk.word_tokenize(x)))\nharry['word count'].head()\n\n0    11\n1    15\n2    14\n3    14\n4    12\nName: word count, dtype: int64\n\n\nTo get the clustering party started I’ll first merge every 4 rows together so that there is more info per row (and since there’s so many rows this will not hurt the sample size too much).\nThis is not trivial! I will group them into individual chapters and then re-split them. I’ll also only take chapters 3 and 4 to save on training time.\n\ncombined_rows = harry.groupby(['book', 'chapter']).agg({'text': ' '.join, 'word count': 'sum'}).reset_index()\ncombined_rows = combined_rows.loc[(combined_rows['book'] == 'Book 3: Prisoner of Azkaban') | (combined_rows['book'] == 'Book 4: Goblet of Fire')]\ncombined_rows.head()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\nword count\n\n\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n4349\n\n\n37\nBook 3: Prisoner of Azkaban\nchap-10\nTHE MARAUDER'S MAP Madam Pomfrey insisted o...\n8973\n\n\n38\nBook 3: Prisoner of Azkaban\nchap-11\nTHE FIREBOLT Harry didn't have a very clear...\n6553\n\n\n39\nBook 3: Prisoner of Azkaban\nchap-12\nTHE PATRONUS Harry knew that Hermione had m...\n6230\n\n\n40\nBook 3: Prisoner of Azkaban\nchap-13\nGRYFFINDOR VERSUS RAVENCLAW It looked like ...\n5398\n\n\n\n\n\n\n\nSeeing how many words are in each row its clear we have a row per chapter. Now to let NLTK do the sentence splitting magic and EXPLODE the rows into their own rows.\n\ncombined_rows['text'] = combined_rows['text'].apply(lambda x: nltk.sent_tokenize(x))\nsentences = combined_rows.explode('text').drop('word count', axis=1)\nsentences.head()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\n\n\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nFor one thing, he hated the summer holidays mo...\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nFor another, he really wanted to do his homewo...\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nAnd he also happened to be a wizard.\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nIt was nearly midnight, and he was lying on hi...\n\n\n\n\n\n\n\n\nsentences.describe()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\n\n\n\n\ncount\n20630\n20630\n20630\n\n\nunique\n2\n37\n18399\n\n\ntop\nBook 4: Goblet of Fire\nchap-9\n.\n\n\nfreq\n14010\n905\n2048\n\n\n\n\n\n\n\nIt’s a little annoying that NLTK thinks a solid number of sentences are a single period so I’ll remove all those rows just in case they ruin my clustering later. (there are some other annoyingly short sentences I removed too)\n\nsentences = sentences.loc[(sentences['text'] != '.') & (sentences['text'] != \".'\") & (sentences['text'] != \"...\")]\nsentences.describe()\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\n\n\n\n\ncount\n18544\n18544\n18544\n\n\nunique\n2\n37\n18396\n\n\ntop\nBook 4: Goblet of Fire\nchap-21\nClunk.\n\n\nfreq\n11925\n851\n9\n\n\n\n\n\n\n\nNow I’ll use some useful tools from NLTK to make columns to use when clustering\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\nsentences['word_count'] = sentences['text'].apply(lambda x: len(word_tokenize(x)))\nsentences['avg_word_length'] = sentences['text'].apply(lambda x: sum(len(word) for word in word_tokenize(x)) / len(word_tokenize(x)))\nsentences['stopwords_count'] = sentences['text'].apply(lambda x: len([word for word in word_tokenize(x) if word.lower() in stop_words]))\nsentences['sentence_length'] = sentences['text'].apply(lambda x: len(x.split()))\n\nimport string\nsentences['punctuation_count'] = sentences['text'].apply(lambda x: len([char for char in x if char in string.punctuation]))\n\n\nsentences.head(1)\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\nword_count\navg_word_length\nstopwords_count\nsentence_length\npunctuation_count\n\n\n\n\n36\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n13\n3.769231\n3\n12\n1\n\n\n\n\n\n\n\nAdding a few more columns that also make use of NLTKs cooler features like polarity.\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n# sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\nsentences['polarity'] = sentences['text'].apply(lambda x: sid.polarity_scores(x)['compound'] )\nsentences['subjectivity_score'] = sentences['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\nfor tag in ['JJ', 'RB', 'NN', 'VB']:\n    sentences[tag + '_count'] = sentences['text'].apply(lambda x: sum(1 for _, pos in nltk.pos_tag(nltk.word_tokenize(x)) if pos == tag))\n\nThe tags ‘JJ’, ‘RB’, ‘NN’, and ‘VB’ correspond to parts of speech adjective, adverb, noun, and verb.\n\nsentences.reset_index(drop=True, inplace=True)\nsentences.head(5)\n\n\n\n\n\n\n\n\nbook\nchapter\ntext\nword_count\navg_word_length\nstopwords_count\nsentence_length\npunctuation_count\npolarity\nsubjectivity_score\nJJ_count\nRB_count\nNN_count\nVB_count\n\n\n\n\n0\nBook 3: Prisoner of Azkaban\nchap-1\nOWL POST Harry Potter was a highly unusual ...\n13\n3.769231\n3\n12\n1\n0.0000\n0.0000\n2\n1\n1\n0\n\n\n1\nBook 3: Prisoner of Azkaban\nchap-1\nFor one thing, he hated the summer holidays mo...\n17\n3.705882\n8\n15\n2\n-0.3818\n-0.3818\n1\n0\n4\n0\n\n\n2\nBook 3: Prisoner of Azkaban\nchap-1\nFor another, he really wanted to do his homewo...\n25\n3.360000\n14\n22\n3\n-0.8990\n-0.8990\n1\n1\n3\n2\n\n\n3\nBook 3: Prisoner of Azkaban\nchap-1\nAnd he also happened to be a wizard.\n9\n3.222222\n5\n8\n1\n0.0000\n0.0000\n0\n1\n1\n1\n\n\n4\nBook 3: Prisoner of Azkaban\nchap-1\nIt was nearly midnight, and he was lying on hi...\n51\n3.941176\n21\n45\n7\n0.3182\n0.3182\n3\n2\n10\n0\n\n\n\n\n\n\n\n\nAnalysis / Results\nI’ll use standarized colums and consider \\(n \\in (2,3,4,5,6,7)\\). I’ll also use standard state 51524 because thats todays date if you were wondering.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nfrom sklearn.metrics import silhouette_samples\n\nX = sentences.drop(['book', 'chapter', 'text' ], axis=1)\n\n\nks = [2,3,4,5,6,7]\n\nfor k in ks:\n    km = make_pipeline(StandardScaler(), KMeans(n_clusters=k, n_init=5, random_state=51524))\n    km.fit(X)\n    y = km[1].labels_\n    sil = silhouette_samples(X, y)\n    ARI = adjusted_rand_score(sentences['book'], y)\n    print(f'k={k}, silhouette={sil.mean()}, ARI={ARI:.4f}')\n\nk=2, silhouette=0.5636526958226444, ARI=-0.0020\nk=3, silhouette=0.4692951542966001, ARI=-0.0016\nk=4, silhouette=0.2630215379212679, ARI=0.0005\nk=5, silhouette=0.034977983880398036, ARI=-0.0003\nk=6, silhouette=0.010108118950209011, ARI=-0.0005\nk=7, silhouette=-0.005114249138976739, ARI=0.0002\n\n\n2 is looking like the most delicious number of clusters and also matches up with my question so I’ll continue on with \\(n=2\\) and visualize its box plot.\n\nkm = make_pipeline(StandardScaler(), KMeans(n_clusters=2, n_init=10, random_state=51524))\nkm.fit(X)\n\ny = km[1].labels_\nsentences[\"cluster\"] = y\nsentences[\"sil\"] = silhouette_samples(X, y)\nsns.catplot(data=sentences, x=\"cluster\", y=\"sil\", kind=\"box\")\n\n\n\n\n\n\n\n\nLooks like cluster 0 is decent except for the crazy amount of outliers and cluster 1 is pretty bad.\nGoing forward with the agglomerative clustering, I’ll only look at \\(n = {2,3,4}\\) since for k-means the other n values were trash and this will save lots of computing time. I’ll also standardize X myself so I don’t need to mess around with pipelines and not even bother with single-linkage because for all the values of \\(n\\) I tested it made \\(n-1\\) singletons.\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nfor col in X.columns:\n    X[col] = StandardScaler().fit_transform(X[[col]])\n\n\nlinkages = [\"ward\", \"complete\", \"average\"]\n\nresults = pd.DataFrame()\n\nfor linkage in linkages:\n    data = X.copy()\n    agg = AgglomerativeClustering(n_clusters=2, linkage=linkage)\n    agg.fit(X)\n    data[\"cluster\"] = agg.labels_\n    data[\"sil\"] = silhouette_samples(X, y)\n    data[\"linkage\"] = linkage\n    results = pd.concat( (results, data) )\nsns.catplot(data=results, x=\"cluster\", y=\"sil\", col='linkage', kind=\"box\")\n\nfor linkage in linkages:\n    data = X.copy()\n    agg = AgglomerativeClustering(n_clusters=3, linkage=linkage)\n    agg.fit(X)\n    data[\"cluster\"] = agg.labels_\n    data[\"sil\"] = silhouette_samples(X, y)\n    data[\"linkage\"] = linkage\n    results = pd.concat( (results, data) )\nsns.catplot(data=results, x=\"cluster\", y=\"sil\", col='linkage', kind=\"box\")\n\nfor linkage in linkages:\n    data = X.copy()\n    agg = AgglomerativeClustering(n_clusters=4, linkage=linkage)\n    agg.fit(X)\n    data[\"cluster\"] = agg.labels_\n    data[\"sil\"] = silhouette_samples(X, y)\n    data[\"linkage\"] = linkage\n    results = pd.concat( (results, data) )\nsns.catplot(data=results, x=\"cluster\", y=\"sil\", col='linkage', kind=\"box\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaking a peek at the ARI\n\nn = [2,3,4]\n\nfor n_clusters in n:\n    for linkage in linkages:\n        agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n        y = agg.fit_predict(X)\n        ARI = adjusted_rand_score(sentences['book'], y)\n        print(f\"ARI for {linkage} linkage with {n_clusters} clusters is {ARI:.4f}\")\n\nARI for ward linkage with 2 clusters is -0.0008\nARI for complete linkage with 2 clusters is 0.0002\nARI for average linkage with 2 clusters is 0.0001\nARI for ward linkage with 3 clusters is -0.0012\nARI for complete linkage with 3 clusters is 0.0002\nARI for average linkage with 3 clusters is 0.0011\nARI for ward linkage with 4 clusters is -0.0016\nARI for complete linkage with 4 clusters is 0.0003\nARI for average linkage with 4 clusters is 0.0012\n\n\nThese ARI scores are pretty bad from a classification standpoint, but it may indicate that Rowling’s writing didn’t truly change from book 3 to 4.\nAs a final MATH219 effort I’ll choose only the top 25% of sentences and do similar speed-run analysis on them since maybe the shorter sentences lack writing style.\n\nX = sentences.drop(['book', 'chapter', 'text'], axis=1).loc[sentences['word_count'] &gt; 26]\nX = StandardScaler().fit_transform(X)\n\nAgglomerative:\n\nn = [2,3,4]\nlinkages = [\"average\", \"single\", \"ward\"]\n\nfor n_clusters in n:\n    for linkage in linkages:\n        agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n        y = agg.fit_predict(X)\n        ARI = adjusted_rand_score(sentences['book'].loc[sentences['word_count'] &gt; 26], y)\n        print(f\"ARI for {linkage} linkage with {n_clusters} clusters is {ARI:.4f}\")\n\nARI for average linkage with 2 clusters is 0.0004\nARI for single linkage with 2 clusters is 0.0004\nARI for ward linkage with 2 clusters is 0.0042\nARI for average linkage with 3 clusters is 0.0008\nARI for single linkage with 3 clusters is 0.0008\nARI for ward linkage with 3 clusters is 0.0012\nARI for average linkage with 4 clusters is 0.0008\nARI for single linkage with 4 clusters is 0.0004\nARI for ward linkage with 4 clusters is 0.0005\n\n\nK-means:\n\nfor n in [2]:\n    km = make_pipeline(StandardScaler(), KMeans(n_clusters=2, n_init=10, random_state=51524))\n    km.fit(X)\n    ARI = adjusted_rand_score(sentences['book'].loc[sentences['word_count'] &gt; 26], km[1].labels_)\n    print(f\"ARI for KMeans with {n} clusters is {ARI:.4f}\")\n\nARI for KMeans with 2 clusters is -0.0070\n\n\n\n\nDiscussion\nThis project originally set out to see if Rowling’s writing on the sentence scale truly changed when she switched her genre from middle grade (books 1-3) to young adult (books 4-7, obviously). Lloyd’s algorithm and a handful of linkage types for agglomerative clustering couldn’t seem to get clusters anywhere near accurate to what book the rows (sentences) came from.\nI originally thought ‘hey maybe its just annoying short sentences kicking around,’ but looking at the top 25% longest sentences revealed all agglomerative clustering linkages couldn’t find the secret (resisting the urge to write jokes about magic) and poor old lloyd (k-means) couldn’t either.\nThe only reasonable takeaway is that on the sentence scale Rowling’s writing did not change in a NLTK implementable, measurable way. Further analysis would need to be done to determine if on a larger scale (maybe paragraph or page) her writing did change to better fit the young adult tag."
  },
  {
    "objectID": "projects/304 honors/index.html",
    "href": "projects/304 honors/index.html",
    "title": "The Best Seat at UD",
    "section": "",
    "text": "This was a project for the honors section of UD’s CHE304 “Random Variability in Chemical Processes.” My main contributions to the report were the literature review, writing the introduction and discussion, and all visualization and formatting. The full report on this study is available below."
  },
  {
    "objectID": "projects/304 honors/index.html#descriptive-statistics",
    "href": "projects/304 honors/index.html#descriptive-statistics",
    "title": "The Best Seat at UD",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis section may feel like figure spam. Figure 1 contains a histogram of the measured desk areas. This shows how the entire population of desk areas is distributed. Most desks are around 200 in\\(^2\\) with very few above 600 in\\(^2\\). The super outlier is Gore 316.\n\n\n\n\n\n\nFigure 1: Histogram of Desk Area (in\\(^2\\))\n\n\n\nSince the vast majority of rooms we measured were labeled by UD’s course inventory, we can consider the different classes of rooms^[Not to be confused with classrooms.} (Table 2) as different populations. The areas of these different populations are visualized as box plots in Figure 2.\n\n\n\n\n\n\nFigure 2: Box Plots of different CIC labels\n\n\n\nThe different styles of rooms have quite different desk areas! Perhaps the auditorium desks are there for a bare minimum, multi-purpose room and the project based learning and case study rooms lend themselves to more desk space for cracking open some cases or problem sets. This is complete conjecture.\nNow we shift focus from desk area towards seat and desk height from the floor. Figure 3 shows the relationship between how high the seat is from the ground and how high the desk surface is from the seat for the different room classes.\n\n\n\n\n\n\nFigure 3: Seat Height vs. Distance Between Desk and Seat by room label\n\n\n\nFigure 4 is a visually busy yet interesting representation of the desk width and length. The red and purple rectangles are values from the literature and clearly a large subset of our data has a smaller width and length than [1]. Not a single desk had a width as thin as [2] which raises questions about what is going on with [2]. Another interesting thing is you can kinda see that the majority of desks will be around that 200 in\\(^2\\) sweet spot in Figure 1 since that 14-17 in range is where a square would have an area of 200 in\\(^2\\) or so.\n\n\n\n\n\n\nFigure 4: Length and Width measurements as Rectangles\n\n\n\nFigure 5 contains information similar to the box plot, but gives more information about the shapes of the desks for the different labels. The auditorium still shows up as an odd one out—a singleton standing lonely outside the rest of the data. Sad sad singleton.\n\n\n\n\n\n\nFigure 5: Desk Length vs. Desk Width, Categorized"
  },
  {
    "objectID": "projects/304 honors/index.html#the-best-seat",
    "href": "projects/304 honors/index.html#the-best-seat",
    "title": "The Best Seat at UD",
    "section": "The Best Seat",
    "text": "The Best Seat\nWhen calculating the seat whose parameters are closest to the literature provided parameters, we need to borrow a wrench from the ml toolbox and account for the difference in spread and magnitude of the different factors. For instance, if we just sloppily found the 2-norm distance between all of our measurements and the literature, the parameters with the largest variance and range would dominate our distance calculation. So we standardized our data by effectively converting all our measurements into t-scores using our sample mean and sample standard deviations for each feature. Equation 1 contains vague information about this standardization in math text therefore it is valid.\n\\[ x \\longrightarrow \\frac{x - \\bar{x}}{s_x}  \\tag{1} \\]\nA brief, nerdy ramble about distance surely fits in here. Many machine learning models and techniques use distance in one way or another. For \\(k\\) nearest neighbors you only know if a neighbor is near if you know how far they are. In fact, we are really searching for the 1-nearest neighbor to our references so it is quite reasonable to treat this as such and standardize the columns. Some playing around could be done with which norm should be used (2-norm, Manhattannorm, \\(\\infty\\)-nom, etc) but no part of this analysis requires something that spicy.\nNow our standardized distance should be able to catch a more accurate best desk instead of being dominated by the parameter with the highest magnitude. Using the chair height, desk height, desk length, and desk width from [2], Colburn 366 is found to be the best seat at UD. Using [1], Penny 209 is found to be the best desk at UD."
  },
  {
    "objectID": "projects/304 honors/index.html#the-worst-seat",
    "href": "projects/304 honors/index.html#the-worst-seat",
    "title": "The Best Seat at UD",
    "section": "The Worst Seat",
    "text": "The Worst Seat\nUsing the same distance metric as discussed above we may also find the worst desks with respect to both references. Compared to [2], Memorial 127 was the worst. Compared to [1] Memorial 48 and Gore 316 tied for worst desk."
  },
  {
    "objectID": "projects/304 honors/index.html#footnotes",
    "href": "projects/304 honors/index.html#footnotes",
    "title": "The Best Seat at UD",
    "section": "Footnotes",
    "text": "Footnotes\n\n\noddly [1] argues chairs with desks connected are favorable to desks where the chair is independent since students tend to lean over the desk while writing instead of keeping their back straight. But for some reason they only mention they made observations and are like “oh yeah they slouched real bad” without ever describing their methods or fully presenting their results…↩︎\nthe dimensions for both references were converted into inches with the appropriate conversion factors before their inclusion in Table 1.↩︎\nThis is in reference to the hit film ‘Morbius’ directed by Daniel Espinosa.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "(insert bio here)"
  }
]